year,publisher,type,platform,model,method,title,doi,url,pdf,ignore,silicon,pubkey,pubname,reserved
2022,IEEE,article,__no_data__,['BERT'],__no_data__,A 28nm 27.5TOPS/W Approximate-Computing-Based Transformer Processor with Asymptotic Sparsity Speculating and Out-of-Order Computing,https://doi.org/10.1109/ISSCC42614.2022.9731686,https://doi.org/10.1109/ISSCC42614.2022.9731686,False,False,True,,IEEE International Solid- State Circuits Conference (ISSCC),DEADBEEF
2022,IEEE,article,__no_data__,['BERT'],__no_data__,A 40nm 5.6TOPS/W 239GOPS/mm2 Self-Attention Processor with Sign Random Projection-based Approximation,https://doi.org/10.1109/ESSCIRC55480.2022.9911343,https://doi.org/10.1109/ESSCIRC55480.2022.9911343,False,False,True,,ESSCIRC 2022- IEEE 48th European Solid State Circuits Conference (ESSCIRC),DEADBEEF
2022,IEEE,article,__no_data__,['BERT'],__no_data__,A Dual-Mode Similarity Search Accelerator based on Embedding Compression for Online Cross-Modal Image-Text Retrieval,https://doi.org/10.1109/FCCM53951.2022.9786159,https://doi.org/10.1109/FCCM53951.2022.9786159,False,False,True,,International Symposium on Field-Programmable Custom Computing Machines (FCCM),DEADBEEF
2022,ACM,article,__no_data__,['BERT'],__no_data__,A Fast and Flexible FPGA-based Accelerator for Natural Language Processing Neural Networks,https://doi.org/10.1145/3564606,https://doi.org/10.1145/3564606,False,False,True,,ACM Transactions on Architecture and Code Optimization,DEADBEEF
2022,arXiv,article,__no_data__,['BERT'],__no_data__,A Fast Post-Training Pruning Framework for Transformers,https://doi.org/10.48550/arXiv.2204.09656,https://doi.org/10.48550/arXiv.2204.09656,False,check,True,,Computer Science > Computation and Language,DEADBEEF
2022,IEEE,article,__no_data__,['BERT'],__no_data__,A Framework for Accelerating Transformer-Based Language Model on ReRAM-Based Architecture,https://doi.org/10.1109/TCAD.2021.3121264,https://doi.org/10.1109/TCAD.2021.3121264,False,False,True,,IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems,DEADBEEF
2021,IEEE/ACM,article,__no_data__,['BERT'],__no_data__,A Framework for Area-efficient Multi-task BERT Execution on ReRAM-based Accelerators,https://doi.org/10.1109/ICCAD51958.2021.9643471,https://doi.org/10.1109/ICCAD51958.2021.9643471,False,False,True,,IEEE/ACM International Conference On Computer Aided Design (ICCAD),DEADBEEF
2021,arXiv,article,__no_data__,['BERT'],__no_data__,A Full-Stack Search Technique for Domain Optimized Deep Learning Accelerators,https://doi.org/10.1145/3503222.3507767,https://doi.org/10.1145/3503222.3507767,False,False,True,,Computer Science > Machine Learning,DEADBEEF
2022,ACM/IEEE,article,__no_data__,['BERT'],__no_data__,A length adaptive algorithm-hardware co-design of transformer on FPGA through sparse attention and dynamic pipelining,https://doi.org/10.1145/3489517.3530585,https://doi.org/10.1145/3489517.3530585,False,False,True,,ACM/IEEE Design Automation Conference,DEADBEEF
2022,MDPI,article,__no_data__,['BERT'],__no_data__,A Lite Romanian BERT: ALR-BERT,https://doi.org/10.3390/computers11040057,https://doi.org/10.3390/computers11040057,False,check,False,,Computers,DEADBEEF
2020,IEEE,article,__no_data__,['BERT'],__no_data__,A Low-Cost Reconfigurable Nonlinear Core for Embedded DNN Applications,https://doi.org/10.1109/ICFPT51103.2020.00014,https://doi.org/10.1109/ICFPT51103.2020.00014,False,False,True,,International Conference on Field-Programmable Technology (ICFPT),DEADBEEF
2021,IEEE,article,__no_data__,['BERT'],__no_data__,A Microcontroller is All You Need: Enabling Transformer Execution on Low-Power IoT Endnodes,https://doi.org/10.1109/COINS51742.2021.9524173,https://doi.org/10.1109/COINS51742.2021.9524173,False,False,True,,IEEE International Conference on Omni-Layer Intelligent Systems (COINS),DEADBEEF
2020,ACM/IEEE,article,__no_data__,['BERT'],__no_data__,A Multi-Neural Network Acceleration Architecture,https://doi.org/10.1109/ISCA45697.2020.00081,https://doi.org/10.1109/ISCA45697.2020.00081,False,check,True,,Annual International Symposium on Computer Architecture (ISCA),DEADBEEF
2019,IEEE,article,__no_data__,['BERT'],__no_data__,A Power Efficient Neural Network Implementation on Heterogeneous FPGA and GPU Devices,https://doi.org/10.1109/IRI.2019.00040,https://doi.org/10.1109/IRI.2019.00040,False,check,True,,International Conference on Information Reuse and Integration for Data Science (IRI),DEADBEEF
2020,MIt%20Press,article,__no_data__,['BERT'],__no_data__,A Primer in BERTology: What We Know About How BERT Works,https://doi.org/10.1162/tacl_a_00349,https://doi.org/10.1162/tacl_a_00349,False,check,check,,Transactions of the Association for Computational Linguistics,DEADBEEF
2021,IEEE,article,__no_data__,['BERT'],__no_data__,A Quantitative Survey of Communication Optimizations in Distributed Deep Learning,https://doi.org/10.1109/MNET.011.2000530,https://doi.org/10.1109/MNET.011.2000530,False,check,check,,IEEE Network,DEADBEEF
2020,IEEE,article,__no_data__,['BERT'],__no_data__,A Reconfigurable DNN Training Accelerator on FPGA,https://doi.org/10.1109/SiPS50750.2020.9195234,https://doi.org/10.1109/SiPS50750.2020.9195234,False,check,check,,IEEE Workshop on Signal Processing Systems (SiPS),DEADBEEF
2022,IEEE,article,__no_data__,['BERT'],__no_data__,A Resource-Saving Energy-Efficient Reconfigurable Hardware Accelerator for BERT-based Deep Neural Network Language Models using FFT Multiplication,https://doi.org/10.1109/ISCAS48785.2022.9937531,https://doi.org/10.1109/ISCAS48785.2022.9937531,False,False,True,,International Symposium on Circuits and Systems (ISCAS),DEADBEEF
2022,IEEE,article,__no_data__,['BERT'],__no_data__,A Self-Attention Network for Deep JSCCM: The Design and FPGA Implementation,https://doi.org/10.1109/GLOBECOM48099.2022.10001518,https://doi.org/10.1109/GLOBECOM48099.2022.10001518,False,False,True,,IEEE Global Communications Conference,DEADBEEF
2019,arXiv,article,__no_data__,['BERT'],__no_data__,A Simple and Effective Approach to Automatic Post-Editing with Transfer Learning,https://doi.org/10.48550/arXiv.1906.06253,https://doi.org/10.48550/arXiv.1906.06253,False,check,check,,Computer Science > Computation and Language,DEADBEEF
2021,arXiv,article,__no_data__,['BERT'],__no_data__,A Study on Token Pruning for ColBERT,https://doi.org/10.48550/arXiv.2112.06540,https://doi.org/10.48550/arXiv.2112.06540,False,check,check,,Computer Science > Information Retrieval,DEADBEEF
2021,arXiv,article,__no_data__,['BERT'],__no_data__,A White Paper on Neural Network Quantization,https://doi.org/10.48550/arXiv.2106.08295,https://doi.org/10.48550/arXiv.2106.08295,False,check,check,,Computer Science > Machine Learning,DEADBEEF
2020,IEEE,article,__no_data__,['BERT'],__no_data__,A^3: Accelerating Attention Mechanisms in Neural Networks with Approximation,https://doi.org/10.1109/HPCA47549.2020.00035,https://doi.org/10.1109/HPCA47549.2020.00035,False,False,True,,International Symposium on High Performance Computer Architecture (HPCA),DEADBEEF
2020,IEEE,article,__no_data__,['BERT'],__no_data__,Emerging Neural Workloads and Their Impact on Hardware,https://doi.org/10.23919/DATE48585.2020.9116435,https://doi.org/10.23919/DATE48585.2020.9116435,False,check,check,,"Design, Automation & Test in Europe Conference & Exhibition (DATE)",DEADBEEF
2021,ACM,article,__no_data__,['BERT'],__no_data__,Accelerated Device Placement Optimization with Contrastive Learning,https://doi.org/10.1145/3472456.3472523,https://doi.org/10.1145/3472456.3472523,False,False,True,,International Conference on Parallel Processing,DEADBEEF
2022,ACM,article,__no_data__,['BERT'],__no_data__,Accelerating attention mechanism on fpgas based on efficient reconfigurable systolic array,https://doi.org/10.1145/3549937,https://doi.org/10.1145/3549937,False,False,True,,ACM Transactions on Embedded Computing Systems,DEADBEEF
2022,ACM,article,__no_data__,['BERT'],__no_data__,Accelerating attention through gradient-based learned runtime pruning,https://doi.org/10.1145/3470496.3527423,https://doi.org/10.1145/3470496.3527423,False,False,True,,International Symposium on Computer Architecture,DEADBEEF
2021,ACM,article,__no_data__,['BERT'],__no_data__,Accelerating bandwidth-bound deep learning inference with main-memory accelerators,https://doi.org/10.1145/3458817.3476146,https://doi.org/10.1145/3458817.3476146,False,False,True,,"International Conference for High Performance Computing, Networking, Storage and Analysis",DEADBEEF
2021,Purdue%20University,article,__no_data__,['BERT'],__no_data__,Accelerating Emerging Neural Workloads,https://doi.org/10.25394/pgs.17139038.v1,https://doi.org/10.25394/pgs.17139038.v1,False,False,True,,Open Access Theses and Dissertations,DEADBEEF
2020,MDPI,article,__no_data__,['BERT'],__no_data__,Accelerating event detection with DGCNN and FPGAS,https://doi.org/10.3390/electronics9101666,https://doi.org/10.3390/electronics9101666,False,False,True,,Electronics,DEADBEEF
2021,IEEE/ACM,article,__no_data__,['BERT'],__no_data__,Accelerating Framework of Transformer by Hardware Design and Model Compression Co-Optimization,https://doi.org/10.1109/ICCAD51958.2021.9643586,https://doi.org/10.1109/ICCAD51958.2021.9643586,False,False,True,,International Conference On Computer Aided Design (ICCAD),DEADBEEF
2022,IEEE,article,__no_data__,['BERT'],__no_data__,Accelerating NLP Tasks on FPGA with Compressed BERT and a Hardware-Oriented Early Exit Method,https://doi.org/10.1109/ISVLSI54635.2022.00092,https://doi.org/10.1109/ISVLSI54635.2022.00092,False,False,True,,IEEE Computer Society Annual Symposium on VLSI (ISVLSI),DEADBEEF
2022,IEEE,article,__no_data__,['BERT'],__no_data__,Accelerating Transformer Networks through Recomposing Softmax Layers,https://doi.org/10.1109/IISWC55918.2022.00018,https://doi.org/10.1109/IISWC55918.2022.00018,http://scale.snu.ac.kr/papers/2022-11-Conference-IISWC-Softmax-recomposition.pdf,False,True,,International Symposium on Workload Characterization (IISWC),DEADBEEF
2021,IEEE,article,__no_data__,['BERT'],__no_data__,Accelerating Transformer-based Deep Learning Models on FPGAs using Column Balanced Block Pruning,https://doi.org/10.1109/ISQED51717.2021.9424344,https://doi.org/10.1109/ISQED51717.2021.9424344,https://wangshusen.github.io/papers/ISQED2021.pdf,False,True,,International Symposium on Quality Electronic Design (ISQED),DEADBEEF
2021,ACM,article,__no_data__,['BERT'],__no_data__,Accommodating Transformer onto FPGA: Coupling the Balanced Model Compression and FPGA-Implementation Optimization,https://doi.org/10.1145/3453688.3461739,https://doi.org/10.1145/3453688.3461739,False,False,True,,Proceedings of the 2021 on Great Lakes Symposium on VLSI,DEADBEEF
2022,IEEE,article,__no_data__,['BERT'],__no_data__,Achieving the Performance of All-Bank In-DRAM PIM With Standard Memory Interface: Memory-Computation Decoupling,https://doi.org/10.1109/ACCESS.2022.3203051,https://doi.org/10.1109/ACCESS.2022.3203051,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9870805,False,True,,IEEE Access,DEADBEEF
2022,IEEE,article,__no_data__,['BERT'],__no_data__,Adaptable Butterfly Accelerator for Attention-based NNs via Hardware and Algorithm Co-design,https://doi.org/10.1109/MICRO56248.2022.00050,https://doi.org/10.1109/MICRO56248.2022.00050,https://arxiv.org/pdf/2209.09570.pdf,False,True,,IEEE/ACM International Symposium on Microarchitecture (MICRO),DEADBEEF
2021,arXiv,article,__no_data__,['BERT'],__no_data__,Adapting by pruning: A case study on BERT,https://doi.org/10.48550/arXiv.2105.03343,https://doi.org/10.48550/arXiv.2105.03343,https://arxiv.org/pdf/2105.03343.pdf,check,check,,Computer Science > Machine Learning,DEADBEEF
2021,ACM,article,__no_data__,['BERT'],__no_data__,"Adaptive Inference through Early-Exit Networks: Design, Challenges and Directions",https://doi.org/10.1145/3469116.3470012,https://doi.org/10.1145/3469116.3470012,False,False,check,,International Workshop on Embedded and Mobile Deep Learning,DEADBEEF
2021,IEEE,article,__no_data__,['BERT'],__no_data__,Adaptive Spatio-Temporal Graph Enhanced Vision-Language Representation for Video QA,https://doi.org/10.1109/TIP.2021.3076556,https://doi.org/10.1109/TIP.2021.3076556,False,False,check,,IEEE Transactions on Image Processing,DEADBEEF
2021,ACM,article,__no_data__,['BERT'],__no_data__,Algorithm-hardware Co-design of Attention Mechanism on FPGA Devices,https://doi.org/10.1145/3477002,https://doi.org/10.1145/3477002,False,False,check,,Transactions on Embedded Computing System,DEADBEEF
2018,IEEE/ACM,article,__no_data__,['BERT'],__no_data__,Algorithm-Hardware Co-Design of Single Shot Detector for Fast Object Detection on FPGAs,https://doi.org/10.1145/3240765.3240775,https://doi.org/10.1145/3240765.3240775,False,False,check,,International Conference on Computer-Aided Design (ICCAD),DEADBEEF
2022,arXiv,article,__no_data__,['BERT'],__no_data__,AlphaTuning: Quantization-Aware Parameter-Efficient Adaptation of Large-Scale Pre-Trained Language Models,https://doi.org/10.48550/arXiv.2210.03858,https://doi.org/10.48550/arXiv.2210.03858,False,False,check,,Computer Science > Machine Learning,DEADBEEF
2022,ACL,article,__no_data__,['BERT'],__no_data__,Alternative non-BERT model choices for the textual classification in low-resource languages and environments,http://dx.doi.org/10.18653/v1/2022.deeplo--1.20,http://dx.doi.org/10.18653/v1/2022.deeplo--1.20,False,False,check,,Proceedings of the Third Workshop on Deep Learning for Low-Resource Natural Language Processing,DEADBEEF
2022,IEEE,article,__no_data__,['BERT'],__no_data__,An Algorithm-Hardware Co-Optimized Framework for Accelerating N:M Sparse Transformers,https://doi.org/10.1109/TVLSI.2022.3197282,https://doi.org/10.1109/TVLSI.2022.3197282,False,False,True,,IEEE Transactions on Very Large Scale Integration (VLSI) Systems (,DEADBEEF
2022,IEEE,article,__no_data__,['BERT'],__no_data__,An Automatic and Efficient BERT Pruning for Edge AI Systems,https://doi.org/10.1109/ISQED54688.2022.9806197,https://doi.org/10.1109/ISQED54688.2022.9806197,False,False,check,,International Symposium on Quality Electronic Design (ISQED),DEADBEEF
2022,IEEE,article,__no_data__,['BERT'],__no_data__,An Efficient Hardware Accelerator for Sparse Transformer Neural Networks,https://doi.org/10.1109/ISCAS48785.2022.9937659,https://doi.org/10.1109/ISCAS48785.2022.9937659,False,False,True,,International Symposium on Circuits and Systems (ISCAS),DEADBEEF
2023,Springer,article,__no_data__,['BERT'],__no_data__,An Efficient Transformer Inference Engine on DSP,https://doi.org/10.1007/978--3--031--22677--9_29,https://doi.org/10.1007/978-3-031-22677-9_29,False,False,True,,International Conference on Algorithms and Architectures for Parallel Processing,DEADBEEF
2020,TheSAI,article,__no_data__,['BERT'],__no_data__,An Empirical Analysis of BERT Embedding for Automated Essay Scoring,https://doi.org/10.14569/ijacsa.2020.0111027,https://doi.org/10.14569/ijacsa.2020.0111027,False,False,True,,International Journal of Advanced Computer Science and Applications,DEADBEEF
2022,IEEE,article,__no_data__,['BERT'],__no_data__,An Energy-Efficient Transformer Processor Exploiting Dynamic Weak Relevances in Global Attention,https://doi.org/10.1109/JSSC.2022.3213521,https://doi.org/10.1109/JSSC.2022.3213521,False,False,True,,Journal of Solid-State Circuits,DEADBEEF
2019,IEEE,article,__no_data__,['BERT'],__no_data__,An Evaluation of Transfer Learning for Classifying Sales Engagement Emails at Large Scale,https://doi.org/10.1109/CCGRID.2019.00069,https://doi.org/10.1109/CCGRID.2019.00069,False,False,True,,"IEEE/ACM International Symposium on Cluster, Cloud and Grid Computing (CCGRID)",DEADBEEF
2022,IEEE,article,__no_data__,['BERT'],__no_data__,An FPGA-Based Transformer Accelerator Using Output Block Stationary Dataflow for Object Recognition Applications,https://doi.org/10.1109/TCSII.2022.3196055,https://doi.org/10.1109/TCSII.2022.3196055,False,False,True,,Transactions on Circuits and Systems II: Express Briefs,DEADBEEF
2020,Springer,article,__no_data__,['BERT'],__no_data__,An investigation on different underlying quantization schemes for pre-trained language models,https://doi.org/10.1007/978--3--030--60450--9_29,https://doi.org/10.1007/978-3-030-60450-9_29,False,False,True,,International Conference on Natural Language Processing and Chinese Computing,DEADBEEF
2022,IEEE,article,__no_data__,['BERT'],__no_data__,Analog-memory-based 14nm Hardware Accelerator for Dense Deep Neural Networks including Transformers,https://doi.org/10.1109/ISCAS48785.2022.9937292,https://doi.org/10.1109/ISCAS48785.2022.9937292,False,False,True,,International Symposium on Circuits and Systems (ISCAS),DEADBEEF
2022,IEEE,article,__no_data__,['BERT'],__no_data__,Answer Fast: Accelerating BERT on the Tensor Streaming Processor,https://doi.org/10.1109/ASAP54787.2022.00022,https://doi.org/10.1109/ASAP54787.2022.00022,False,False,True,,"International Conference on Application-specific Systems, Architectures and Processors (ASAP)",DEADBEEF
2022,IEEE,article,__no_data__,['BERT'],__no_data__,ANT: Exploiting Adaptive Numerical Data Type for Low-bit Deep Neural Network Quantization,https://doi.org/10.1109/MICRO56248.2022.00095,https://doi.org/10.1109/MICRO56248.2022.00095,False,False,True,,IEEE/ACM International Symposium on Microarchitecture (MICRO),DEADBEEF
2022,Elsevier,article,__no_data__,['BERT'],__no_data__,APT: The master-copy-free training method for quantised neural network on edge devices,https://doi.org/10.1016/j.jpdc.2022.04.005,https://doi.org/10.1016/j.jpdc.2022.04.005,False,False,True,,Journal of Parallel and Distributed Computing,DEADBEEF
2021,IEEE,article,__no_data__,['BERT'],__no_data__,Aquabolt-XL: Samsung HBM2-PIM with in-memory processing for ML accelerators and beyond,https://doi.org/10.1109/HCS52781.2021.9567191,https://doi.org/10.1109/HCS52781.2021.9567191,False,False,True,,IEEE Hot Chips 33 Symposium (HCS),DEADBEEF
2020,IEEE,article,__no_data__,['BERT'],__no_data__,ATT: A Fault-Tolerant ReRAM Accelerator for Attention-based Neural Networks,https://doi.org/10.1109/ICCD50377.2020.00047,https://doi.org/10.1109/ICCD50377.2020.00047,False,False,True,,International Conference on Computer Design (ICCD),DEADBEEF
2021,PlosOne,article,__no_data__,['BERT'],__no_data__,AUBER: Automated BERT regularization,https://doi.org/10.1371/journal.pone.0253241,https://doi.org/10.1371/journal.pone.0253241,False,False,True,,Plos one,DEADBEEF
2022,arXiv,article,__no_data__,['BERT'],__no_data__,Auto-ViT-Acc: An FPGA-Aware Automatic Acceleration Framework for Vision Transformer with Mixed-Scheme Quantization,https://doi.org/10.48550/arXiv.2208.05163,https://doi.org/10.48550/arXiv.2208.05163,False,False,True,,Computer Science > Computer Vision and Pattern Recognition,DEADBEEF
2021,arXiv,article,__no_data__,['BERT'],__no_data__,Automatic Mixed-Precision Quantization Search of BERT,https://doi.org/10.24963/ijcai.2021/472,https://doi.org/10.24963/ijcai.2021/472,False,False,True,,Computer Science > Computation and Language,DEADBEEF
2022,IEEE,article,__no_data__,['BERT'],__no_data__,Balance Multi-Head Attention based on Software and Hardware Co-design,https://doi.org/10.1109/CSCloud--EdgeCom54986.2022.00018,https://doi.org/10.1109/CSCloud-EdgeCom54986.2022.00018,False,False,True,,International Conference on Edge Computing and Scalable Cloud (EdgeCom),DEADBEEF
2022,arXiv,article,__no_data__,['BERT'],__no_data__,BEBERT: Efficient and robust binary ensemble BERT,https://doi.org/10.48550/arXiv.2210.15976,https://doi.org/10.48550/arXiv.2210.15976,False,False,True,,Computer Science > Computation and Language,DEADBEEF
2021,arXiv,article,__no_data__,['BERT'],__no_data__,BERMo: What can BERT learn from ELMo?,https://doi.org/10.48550/arXiv.2110.15802,https://doi.org/10.48550/arXiv.2110.15802,False,False,True,,Computer Science > Computation and Language,DEADBEEF
2021,IEEE,article,__no_data__,['BERT'],__no_data__,BERT Model for Classification of Fake News using the Cloud Processing Capacity,https://doi.org/10.1109/R10--HTC53172.2021.9641632,https://doi.org/10.1109/R10-HTC53172.2021.9641632,False,False,True,,IEEE 9th Region 10 Humanitarian Technology Conference (R10-HTC),DEADBEEF
2022,LUT%20University,article,__no_data__,['BERT'],__no_data__,BERT model optimization methods for inference: a comparative study of five alternative BERT-model implementations,https://urn.fi/URN:NBN:fi--fe2022121270782,https://urn.fi/URN:NBN:fi-fe2022121270782,False,False,True,,"School of Engineering Science, Tuotantotalous",DEADBEEF
2022,arXiv,article,__no_data__,['BERT'],__no_data__,BERT on a Data Diet: Finding Important Examples by Gradient-Based Pruning,https://doi.org/10.48550/arXiv.2211.05610,https://doi.org/10.48550/arXiv.2211.05610,False,False,True,,Computer Science > Computation and Language,DEADBEEF
2021,arXiv,article,__no_data__,['BERT'],__no_data__,Bertinho: Galician BERT representations,https://doi.org/10.48550/arXiv.2103.13799,https://doi.org/10.48550/arXiv.2103.13799,False,False,True,,Computer Science > Computation and Language,DEADBEEF
2022,IEEE,article,__no_data__,['BERT'],__no_data__,BERTPerf: Inference Latency Predictor for BERT on ARM big.LITTLE Multi-Core Processors,https://doi.org/10.1109/SiPS55645.2022.9919203,https://doi.org/10.1109/SiPS55645.2022.9919203,False,False,True,,IEEE Workshop on Signal Processing Systems (SiPS),DEADBEEF
2021,ACL,article,__no_data__,['BERT'],__no_data__,BERxiT: Early exiting for BERT with better fine-tuning and extension to regression,http://dx.doi.org/10.18653/v1/2021.--eacl--main.8,http://dx.doi.org/10.18653/v1/2021.eacl-main.8,False,False,True,,Association%20for%20Computational%20Linguistics,DEADBEEF
2021,arXiv,article,__no_data__,['BERT'],__no_data__,Beyond preserved accuracy: Evaluating loyalty and robustness of BERT compression,https://doi.org/10.48550/arXiv.2109.03228,https://doi.org/10.48550/arXiv.2109.03228,False,False,True,,Computer Science > Computation and Language,DEADBEEF
2022,arXiv,article,__no_data__,['BERT'],__no_data__,BiBERT: Accurate Fully Binarized BERT,https://doi.org/10.48550/arXiv.2203.06390,https://doi.org/10.48550/arXiv.2203.06390,False,False,True,,Computer Science > Computation and Language,DEADBEEF
2022,arXiv,article,__no_data__,['BERT'],__no_data__,Bigger&Faster: Two-stage Neural Architecture Search for Quantized Transformer Models,https://doi.org/10.48550/arXiv.2209.12127,https://doi.org/10.48550/arXiv.2209.12127,False,False,True,,Computer Science > Machine Learning,DEADBEEF
2021,IEEE,article,__no_data__,['BERT'],__no_data__,Binary Complex Neural Network Acceleration on FPGA : (Invited Paper),https://doi.org/10.1109/ASAP52443.2021.00021,https://doi.org/10.1109/ASAP52443.2021.00021,False,False,True,,"International Conference on Application-specific Systems, Architectures and Processors (ASAP)",DEADBEEF
2020,arXiv,article,__no_data__,['BERT'],__no_data__,Binarybert: Pushing the limit of bert quantization,https://doi.org/10.48550/arXiv.2012.15701,https://doi.org/10.48550/arXiv.2012.15701,False,False,True,,Computer Science > Computation and Language,DEADBEEF
2021,Springer,article,__no_data__,['BERT'],__no_data__,Biomedical Named Entity Recognition at Scale,https://doi.org/10.1007/978--3--030--68763--2_48,https://doi.org/10.1007/978-3-030-68763-2_48,False,False,True,,International Conference on Pattern Recognition,DEADBEEF
2022,arXiv,article,__no_data__,['BERT'],__no_data__,BiT: Robustly Binarized Multi-distilled Transformer,https://doi.org/10.48550/arXiv.2205.13016,https://doi.org/10.48550/arXiv.2205.13016,False,False,True,,Computer Science > Machine Learning,DEADBEEF
2021,arXiv,article,__no_data__,['BERT'],__no_data__,Block pruning for faster transformers,https://doi.org/10.48550/arXiv.2109.04838,https://doi.org/10.48550/arXiv.2109.04838,False,False,True,,Computer Science > Machine Learning,DEADBEEF
2022,arXiv,article,__no_data__,['BERT'],__no_data__,Boosting Distributed Training Performance of the Unpadded BERT Model,https://doi.org/10.48550/arXiv.2208.08124,https://doi.org/10.48550/arXiv.2208.08124,False,False,True,,"Computer Science > Distributed, Parallel, and Cluster Computing",DEADBEEF
2020,ACM,article,__no_data__,['BERT'],__no_data__,Capuchin: Tensor-based GPU Memory Management for Deep Learning,https://doi.org/10.1145/3373376.3378505,https://doi.org/10.1145/3373376.3378505,False,False,True,,International Conference on Architectural Support for Programming Languages and Operating Systems,DEADBEEF
2020,arXiv,article,__no_data__,['BERT'],__no_data__,CATBERT: Context-Aware Tiny BERT for Detecting Social Engineering Emails,https://doi.org/10.48550/arXiv.2010.03484,https://doi.org/10.48550/arXiv.2010.03484,False,False,True,,Computer Science > Cryptography and Security,DEADBEEF
2020,arXiv,article,__no_data__,['BERT'],__no_data__,CatBERT: Context-Aware Tiny BERT for Detecting Targeted Social Engineering Emails,https://doi.org/10.48550/arXiv.2010.03484,https://doi.org/10.48550/arXiv.2010.03484,False,False,True,,Computer Science > Cryptography and Security,DEADBEEF
2023,arXiv,article,__no_data__,['BERT'],__no_data__,CHARM: Composing Heterogeneous Accelerators for Matrix Multiply on Versal ACAP Architecture,https://doi.org/10.48550/arXiv.2301.02359,https://doi.org/10.48550/arXiv.2301.02359,False,False,True,,Computer Science > Hardware Architecture,DEADBEEF
2020,ACM,article,__no_data__,['BERT'],__no_data__,ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT,https://doi.org/10.1145/3397271.3401075,https://doi.org/10.1145/3397271.3401075,False,False,True,,International ACM SIGIR Conference on Research and Development in Information Retrieval,DEADBEEF
2020,Springer,article,__no_data__,['BERT'],__no_data__,Combining Feature Selection Methods with BERT: An In-depth Experimental Study of Long Text Classification,https://doi.org/10.1007/978--3--030--67537--0_34,https://doi.org/10.1007/978-3-030-67537-0_34,False,False,True,,"International Conference on Collaborative Computing: Networking, Applications and Worksharing",DEADBEEF
2022,arXiv,article,__no_data__,['BERT'],__no_data__,Compact Token Representations with Contextual Quantization for Efficient Document Re-ranking,https://doi.org/10.48550/arXiv.2203.15328,https://doi.org/10.48550/arXiv.2203.15328,False,False,True,,Computer Science > Information Retrieval,DEADBEEF
2020,MDPI,article,__no_data__,['BERT'],__no_data__,Comparison of Deep Learning Models and Various Text Pre-Processing Techniques for the Toxic Comments Classification,https://doi.org/10.3390/app10238631,https://doi.org/10.3390/app10238631,False,False,True,,Natural Language Processing: Emerging Neural Approaches and Applications,DEADBEEF
2020,arXiv,article,__no_data__,['BERT'],__no_data__,Compressing BERT: Studying the Effects of Weight Pruning on Transfer Learning,https://doi.org/10.48550/arXiv.2002.08307,https://doi.org/10.48550/arXiv.2002.08307,False,False,True,,Computer Science > Computation and Language,DEADBEEF
2021,MIT%20Press,article,__no_data__,['BERT'],__no_data__,Compressing Large-Scale Transformer-Based Models: A Case Study on BERT,https://doi.org/10.1162/tacl_a_00413,https://doi.org/10.1162/tacl_a_00413,False,False,True,,Transactions of the Association for Computational Linguistics,DEADBEEF
2022,arXiv,article,__no_data__,['BERT'],__no_data__,Compressing Pre-trained Transformers via Low-Bit NxM Sparsity for Natural Language Understanding,https://doi.org/10.48550/arXiv.2206.15014,https://doi.org/10.48550/arXiv.2206.15014,False,False,True,,Computer Science > Computation and Language,DEADBEEF
2020,ACM,article,__no_data__,['BERT'],__no_data__,Compression of deep learning models for NLP,https://doi.org/10.1145/3340531.3412171,https://doi.org/10.1145/3340531.3412171,False,True,True,,ACM International Conference on Information & Knowledge Managemen,DEADBEEF
2022,arXiv,article,__no_data__,['BERT'],__no_data__,Compression of Generative Pre-trained Language Models via Quantization,https://doi.org/10.48550/arXiv.2203.10705,https://doi.org/10.48550/arXiv.2203.10705,False,False,True,,Computer Science > Computation and Language,DEADBEEF
2022,MDPI,article,__no_data__,['BERT'],__no_data__,CONNA: Configurable Matrix Multiplication Engine for Neural Network Acceleration,https://doi.org/10.3390/electronics11152373,https://doi.org/10.3390/electronics11152373,False,True,True,,Electronics,DEADBEEF
2022,arXiv,article,__no_data__,['BERT'],__no_data__,CPSAA: Accelerating Sparse Attention using Crossbar-based Processing-In-Memory Architecture,https://doi.org/10.48550/arXiv.2210.06696,https://doi.org/10.48550/arXiv.2210.06696,False,False,True,,Computer Science > Hardware Architecture,DEADBEEF
2021,Springer,article,__no_data__,['BERT'],__no_data__,DAP-BERT: Differentiable Architecture Pruning of BERT,https://doi.org/10.1007/978--3--030--92185--9_30,https://doi.org/10.1007/978-3-030-92185-9_30,False,False,True,,International Conference on Neural Information Processing,DEADBEEF
2020,IEEE,article,__no_data__,['BERT'],__no_data__,Deep Learning Acceleration with Neuron-to-Memory Transformation,https://doi.org/10.1109/HPCA47549.2020.00011,https://doi.org/10.1109/HPCA47549.2020.00011,False,False,True,,International Symposium on High Performance Computer Architecture (HPCA),DEADBEEF
2021,arXiv,article,__no_data__,['BERT'],__no_data__,Demystifying BERT: Implications for Accelerator Design,https://doi.org/10.48550/arXiv.2104.08335,https://doi.org/10.48550/arXiv.2104.08335,False,False,True,,Computer Science > Hardware Architecture,DEADBEEF
2022,IEEE,article,__no_data__,['BERT'],__no_data__,Demystifying BERT: System Design Implications,https://doi.org/10.1109/IISWC55918.2022.00033,https://doi.org/10.1109/IISWC55918.2022.00033,False,False,check,,International Symposium on Workload Characterization (IISWC),DEADBEEF
2022,IEEE,article,__no_data__,['BERT'],__no_data__,DFX: A Low-latency Multi-FPGA Appliance for Accelerating Transformer-based Text Generation,https://doi.org/10.1109/MICRO56248.2022.00051,https://doi.org/10.1109/MICRO56248.2022.00051,False,False,True,,International Symposium on Microarchitecture (MICRO),DEADBEEF
2022,Elsevier,article,__no_data__,['BERT'],__no_data__,DiVIT: Algorithm and architecture co-design of differential attention in vision transformer,https://doi.org/10.1016/j.sysarc.2022.102520,https://doi.org/10.1016/j.sysarc.2022.102520,False,False,True,,Journal of Systems Architecture,DEADBEEF
2022,ACM,article,__no_data__,['BERT'],__no_data__,DOTA: Detect and Omit Weak Attentions for Scalable Transformer Acceleration,https://doi.org/10.1145/3503222.3507738,https://doi.org/10.1145/3503222.3507738,False,False,True,,ACM International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS),DEADBEEF
2022,arXiv,article,__no_data__,['BERT'],__no_data__,DQ-BART: Efficient Sequence-to-Sequence Model via Joint Distillation and Quantization,https://doi.org/10.48550/arXiv.2203.11239,https://doi.org/10.48550/arXiv.2203.11239,False,False,True,,Computer Science > Computation and Language,DEADBEEF
2023,IEEE,article,__no_data__,['BERT'],__no_data__,DTATrans: Leveraging Dynamic Token-Based Quantization With Accuracy Compensation Mechanism for Efficient Transformer Architecture,https://doi.org/10.1109/TCAD.2022.3181541,https://doi.org/10.1109/TCAD.2022.3181541,False,False,True,,IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems,DEADBEEF
2022,IEEE,article,__no_data__,['BERT'],__no_data__,DTQAtten: Leveraging Dynamic Token-based Quantization for Efficient Attention Architecture,https://doi.org/10.23919/DATE54114.2022.9774692,https://doi.org/10.23919/DATE54114.2022.9774692,False,False,True,,"Design, Automation & Test in Europe Conference & Exhibition (DATE)",DEADBEEF
2022,IEEE,article,__no_data__,['BERT'],__no_data__,Dynamic Precision Analog Computing for Neural Networks,https://doi.org/10.1109/JSTQE.2022.3218019,https://doi.org/10.1109/JSTQE.2022.3218019,False,False,True,,IEEE Journal of Selected Topics in Quantum Electronics,DEADBEEF
2021,arXiv,article,__no_data__,['BERT'],__no_data__,Dynamic-TinyBERT: Boost TinyBERT's Inference Efficiency by Dynamic Sequence Length,https://doi.org/10.48550/arXiv.2111.09645,https://doi.org/10.48550/arXiv.2111.09645,False,False,True,,Computer Science > Computation and Language,DEADBEEF
2021,IEEE,article,__no_data__,['BERT'],__no_data__,EAGLE: Expedited Device Placement with Automatic Grouping for Large Models,https://doi.org/10.1109/IPDPS49936.2021.00068,https://doi.org/10.1109/IPDPS49936.2021.00068,False,False,True,,International Parallel and Distributed Processing Symposium (IPDPS),DEADBEEF
2020,arXiv,article,__no_data__,['BERT'],__no_data__,Earlybert: Efficient bert training via early-bird lottery tickets,https://doi.org/10.48550/arXiv.2101.00063,https://doi.org/10.48550/arXiv.2101.00063,False,True,True,,Computer Science > Computation and Language,DEADBEEF
2021,ACL,article,__no_data__,['BERT'],__no_data__,EBERT: Efficient BERT Inference with Dynamic Structured Pruning,http://dx.doi.org/10.18653/v1/2021.findings--acl.425,http://dx.doi.org/10.18653/v1/2021.findings-acl.425,False,False,True,,ACL Findings,DEADBEEF
2021,IEEE/ACM,article,__no_data__,['BERT'],__no_data__,EdgeBERT: Sentence-Level Energy Optimizations for Latency-Aware Multi-Task NLP Inference,https://doi.org/10.1145/3466752.3480095,https://doi.org/10.1145/3466752.3480095,False,False,True,,IEEE/ACM International Symposium on Microarchitecture,DEADBEEF
2022,MDPI,article,__no_data__,['BERT'],__no_data__,EFA-Trans: An Efficient and Flexible Acceleration Architecture for Transformers,https://doi.org/10.3390/electronics11213550,https://doi.org/10.3390/electronics11213550,False,False,True,,Electronics,DEADBEEF
2020,MIT,article,__no_data__,['BERT'],__no_data__,Efficient algorithms and hardware for natural language processing,https://hdl.handle.net/1721.1/127440,https://hdl.handle.net/1721.1/127440,False,False,True,,MIT Master's Thesis,DEADBEEF
2022,arXiv,article,__no_data__,['BERT'],__no_data__,Efficient Document Retrieval by End-to-End Refining and Quantizing BERT Embedding with Contrastive Product Quantization,https://arxiv.org/abs/2210.17170v1,https://arxiv.org/abs/2210.17170v1,False,True,True,,Computer Science > Information Retrieval,DEADBEEF
2020,arXiv,article,__no_data__,['BERT'],__no_data__,Efficient transformer-based large scale language representations using hardware-friendly block structured pruning,https://doi.org/10.48550/arXiv.2009.08065,https://doi.org/10.48550/arXiv.2009.08065,False,False,True,,Computer Science > Computation and Language,DEADBEEF
2022,ProQuest,article,__no_data__,['BERT'],__no_data__,Elastic Processing and Hardware Architectures for Machine Learning,3e9f91ca96ba3320587da2bbec561a2b/,https://www.proquest.com/openview/3e9f91ca96ba3320587da2bbec561a2b/,False,False,True,,UC Santa Barbara Ph.D. Dissertation,DEADBEEF
2021,IEEE,article,__no_data__,['BERT'],__no_data__,"ELSA: Hardware-Software co-design for efficient, lightweight self-attention mechanism in neural networks",https://doi.org/10.1109/ISCA52012.2021.00060,https://doi.org/10.1109/ISCA52012.2021.00060,False,False,True,,International Symposium on Computer Architecture (ISCA),DEADBEEF
2022,arXiv,article,__no_data__,['BERT'],__no_data__,Empirical Evaluation of Post-Training Quantization Methods for Language Tasks,https://doi.org/10.48550/arXiv.2210.16621,https://doi.org/10.48550/arXiv.2210.16621,False,True,True,,Computer Science > Computation and Language,DEADBEEF
2022,arXiv,article,__no_data__,['BERT'],__no_data__,Enabling and Accelerating Dynamic Vision Transformer Inference for Real-Time Applications,https://doi.org/10.48550/arXiv.2212.02687,https://doi.org/10.48550/arXiv.2212.02687,False,False,True,,Computer Science > Computer Vision and Pattern Recognition,DEADBEEF
2022,IEEE,article,__no_data__,['BERT'],__no_data__,Enabling Efficient Large-Scale Deep Learning Training with Cache Coherent Disaggregated Memory Systems,https://doi.org/10.1109/HPCA53966.2022.00018,https://doi.org/10.1109/HPCA53966.2022.00018,False,False,True,,International Symposium on High-Performance Computer Architecture (HPCA),DEADBEEF
2021,ACM,article,__no_data__,['BERT'],__no_data__,Enabling energy-efficient DNN training on hybrid GPU-FPGA accelerators,https://doi.org/10.1145/3447818.3460371,https://doi.org/10.1145/3447818.3460371,False,False,True,,Proceedings of the ACM International Conference on Supercomputing,DEADBEEF
2022,IEEE,article,__no_data__,['BERT'],__no_data__,Enabling Energy-Efficient Inference for Self-Attention Mechanisms in Neural Networks,https://doi.org/10.1109/AICAS54282.2022.9869924,https://doi.org/10.1109/AICAS54282.2022.9869924,False,False,True,,International Conference on Artificial Intelligence Circuits and Systems (AICAS),DEADBEEF
2022,ACM,article,__no_data__,['BERT'],__no_data__,Enabling fast uncertainty estimation: accelerating bayesian transformers via algorithmic and hardware optimizations,https://doi.org/10.1145/3489517.3530451,https://doi.org/10.1145/3489517.3530451,False,False,True,,ACM/IEEE Design Automation Conference (DAC),DEADBEEF
2022,ACM,article,__no_data__,['BERT'],__no_data__,Enabling Fast Uncertainty Estimation: Exploiting Structured Sparsity in Bayesian Transformers,https://doi.org/10.1145/3489517.3530451,https://spiral.imperial.ac.uk/bitstream/10044/1/96226/2/dac22hf3_final_bayesatt.pdf,False,False,True,,ACM/IEEE Design Automation Conference (DAC),DEADBEEF
2021,IEEE,article,__no_data__,['BERT'],__no_data__,Enabling One-Size-Fits-All Compilation Optimization for Inference Across Machine Learning Computers,https://doi.org/10.1109/TC.2021.3128266,https://doi.org/10.1109/TC.2021.3128266,False,check,True,,IEEE Transactions on Computers,DEADBEEF
2021,IEEE,article,__no_data__,['BERT'],__no_data__,Energy efficiency boost in the AI-infused POWER10 processor,https://doi.org/10.1109/ISCA52012.2021.00012,https://doi.org/10.1109/ISCA52012.2021.00012,False,False,True,,International Symposium on Computer Architecture (ISCA),DEADBEEF
2023,MDPI,article,__no_data__,['BERT'],__no_data__,ENEX-FP: A BERT-Based Address Recognition Model,https://doi.org/10.3390/electronics12010209,https://doi.org/10.3390/electronics12010209,False,True,True,,Electronics,DEADBEEF
2022,Springer,article,__no_data__,['BERT'],__no_data__,Ensemble Model Compression for Fast and Energy-Efficient Ranking on FPGAs,https://doi.org/10.1007/978--3--030--99736--6_18,https://doi.org/10.1007/978-3-030-99736-6_18,False,False,True,,European Conference on Information Retrieval (ECIR),DEADBEEF
2022,IEEE,article,__no_data__,['BERT'],__no_data__,Extending the ONNX Runtime Framework for the Processing-in-Memory Execution,https://doi.org/10.1109/ICEIC54506.2022.9748444,https://doi.org/10.1109/ICEIC54506.2022.9748444,False,False,True,,"International Conference on Electronics, Information, and Communication (ICEIC)",DEADBEEF
2022,arXiv,article,__no_data__,['BERT'],__no_data__,Extreme Compression for Pre-trained Transformers Made Simple and Efficient,https://doi.org/10.48550/arXiv.2206.01859,https://doi.org/10.48550/arXiv.2206.01859,False,check,True,,Computer Science > Computation and Language,DEADBEEF
2020,Springer,article,__no_data__,['BERT'],__no_data__,FARM: A flexible accelerator for recurrent and memory augmented neural networks,https://doi.org/10.1007/s11265--020--01555--w,https://doi.org/10.1007/s11265-020-01555-w,False,False,True,,Journal of Signal Processing Systems,DEADBEEF
2022,None,article,__no_data__,['BERT'],__no_data__,Fast Heterogeneous Task Mapping for Reducing Edge DNN Latency,https://doi.org/10.1109/ASAP54787.2022.00020,https://doi.org/10.1109/ASAP54787.2022.00020,False,False,True,,"International Conference on Application-specific Systems, Architectures and Processors (ASAP)",DEADBEEF
2020,arXiv,article,__no_data__,['BERT'],__no_data__,Fastformers: Highly efficient transformer models for natural language understanding,https://doi.org/10.48550/arXiv.2010.13382,https://doi.org/10.48550/arXiv.2010.13382,False,False,True,,Computer Science > Computation and Language,DEADBEEF
2022,ACM/SIGDA,article,__no_data__,['BERT'],__no_data__,"FILM-QNN: Efficient FPGA Acceleration of Deep Neural Networks with Intra-Layer, Mixed-Precision Quantization",https://doi.org/10.1145/3490422.3502364,https://doi.org/10.1145/3490422.3502364,False,False,True,,International Symposium on Field-Programmable Gate Arrays,DEADBEEF
2022,arXiv,article,__no_data__,['BERT'],__no_data__,Fine-and Coarse-Granularity Hybrid Self-Attention for Efficient BERT,https://doi.org/10.48550/arXiv.2203.09055,https://doi.org/10.48550/arXiv.2203.09055,False,check,True,,Computer Science > Computation and Language,DEADBEEF
2021,IEEE,article,__no_data__,['BERT'],__no_data__,Fixed-point Quantization for Vision Transformer,https://doi.org/10.1109/CAC53003.2021.9728246,https://doi.org/10.1109/CAC53003.2021.9728246,False,False,True,,China Automation Congress (CAC),DEADBEEF
2021,IEEE,article,__no_data__,['BERT'],__no_data__,FlexACC: A Programmable Accelerator with Application-Specific ISA for Flexible Deep Neural Network Inference,https://doi.org/10.1109/ASAP52443.2021.00046,https://doi.org/10.1109/ASAP52443.2021.00046,False,False,True,,"International Conference on Application-specific Systems, Architectures and Processors (ASAP)",DEADBEEF
2022,ACM/IEEE,article,__no_data__,['BERT'],__no_data__,FPGA-aware automatic acceleration framework for vision transformer with mixed-scheme quantization: late breaking results,https://doi.org/10.1145/3489517.3530618,https://doi.org/10.1145/3489517.3530618,False,False,True,,ACM/IEEE Design Automation Conference,DEADBEEF
2022,IOS%20Press,article,__no_data__,['BERT'],__no_data__,FPGA-based design and implementation of the location attention mechanism in neural networks,https://doi.org/10.3233/JIFS--212273,https://doi.org/10.3233/JIFS-212273,False,False,True,,Journal of Intelligent & Fuzzy Systems,DEADBEEF
2022,AAAI,article,__no_data__,['BERT'],__no_data__,From dense to sparse: Contrastive pruning for better pre-trained language model compression,https://doi.org/10.1609/aaai.v36i10.21408,https://doi.org/10.1609/aaai.v36i10.21408,False,check,True,,AAAI Technical Track on Speech and Natural Language Processing,DEADBEEF
2020,ACM/IEEE,article,__no_data__,['BERT'],__no_data__,FTRANS: energy-efficient acceleration of transformers using FPGA,https://doi.org/10.1145/3370748.3406567,https://doi.org/10.1145/3370748.3406567,False,False,True,,ACM/IEEE International Symposium on Low Power Electronics and Design,DEADBEEF
2022,IEEE,article,__no_data__,['BERT'],__no_data__,Future Scaling of Memory Hierarchy for Tensor Cores and Eliminating Redundant Shared Memory Traffic Using Inter-Warp Multicastin,https://doi.org/10.1109/TC.2022.3207134,https://doi.org/10.1109/TC.2022.3207134,False,False,True,,IEEE Transactions on Computers,DEADBEEF
2021,IEEE,article,__no_data__,['BERT'],__no_data__,Gemmini: Enabling systematic deep-learning architecture evaluation via full-stack integration,https://doi.org/10.1109/DAC18074.2021.9586216,https://doi.org/10.1109/DAC18074.2021.9586216,False,False,True,,ACM/IEEE Design Automation Conference (DAC),DEADBEEF
2021,IEEE/ACM,article,__no_data__,['BERT'],__no_data__,Gobo: Quantizing attention-based nlp models for low latency and energy efficient inference,https://doi.org/10.1109/MICRO50266.2020.00071,https://doi.org/10.1109/MICRO50266.2020.00071,False,False,True,,IEEE/ACM International Symposium on Microarchitecture (MICRO),DEADBEEF
2022,Elsevier,article,__no_data__,['BERT'],__no_data__,Greedy-layer pruning: Speeding up transformer models for natural language processing,https://doi.org/10.1016/j.patrec.2022.03.023,https://doi.org/10.1016/j.patrec.2022.03.023,False,False,True,,Pattern Recognition Letters,DEADBEEF
2022,ACM/IEEE,article,__no_data__,['BERT'],__no_data__,GuardNN: secure accelerator architecture for privacy-preserving deep learning,https://doi.org/10.1145/3489517.3530439,https://doi.org/10.1145/3489517.3530439,False,False,True,,ACM/IEEE Design Automation Conference,DEADBEEF
2023,IEEE,article,__no_data__,['BERT'],__no_data__,HAMMER: Hardware-friendly Approximate Computing for Self-attention with Mean-redistribution and Linearization,https://doi.org/10.1109/LCA.2022.3233832,https://doi.org/10.1109/LCA.2022.3233832,False,False,True,,IEEE Computer Architecture Letters,DEADBEEF
2022,ACM,article,__no_data__,['BERT'],__no_data__,Handling heavy-tailed input of transformer inference on GPUs,https://doi.org/10.1145/3524059.3532372,https://doi.org/10.1145/3524059.3532372,False,False,True,,ACM International Conference on Supercomputing (ICS),DEADBEEF
2021,IEEE,article,__no_data__,['BERT'],__no_data__,Hardware Acceleration of Fully Quantized BERT for Efficient Natural Language Processing,https://doi.org/10.23919/DATE51398.2021.9474043,https://doi.org/10.23919/DATE51398.2021.9474043,False,False,True,,"Design, Automation & Test in Europe Conference & Exhibition (DATE)",DEADBEEF
2021,IEEE,article,__no_data__,['BERT'],__no_data__,Hardware acceleration of sparse and irregular tensor computations of ml models: A survey and insights,https://doi.org/10.1109/JPROC.2021.3098483,https://doi.org/10.1109/JPROC.2021.3098483,False,False,True,,Proceedings of the IEEE,DEADBEEF
2022,IEEE,article,__no_data__,['BERT'],__no_data__,Hardware Acceleration of Transformer Networks using FPGAs,https://doi.org/10.1109/PACET56979.2022.9976354,https://doi.org/10.1109/PACET56979.2022.9976354,False,False,True,,Panhellenic Conference on Electronics & Telecommunications (PACET),DEADBEEF
2020,IEEE,article,__no_data__,['BERT'],__no_data__,Hardware accelerator for multi-head attention and position-wise feed-forward in the transformer,https://doi.org/10.1109/SOCC49529.2020.9524802,https://doi.org/10.1109/SOCC49529.2020.9524802,False,False,True,,International System-on-Chip Conference (SOCC),DEADBEEF
2022,Springer,article,__no_data__,['BERT'],__no_data__,Hardware and Software Co-design for Soft Switch in ViT Variants Processing Unit,https://doi.org/10.1007/978--3--031--10989--8_55,https://doi.org/10.1007/978-3-031-10989-8_55,False,False,True,,"International Conference on Knowledge Science, Engineering and Management",DEADBEEF
2022,None,article,__no_data__,['BERT'],__no_data__,Hardware and Software Co-optimization for Windows Attention,https://doi.org/10.1007/978--3--031--10989--8_52,https://doi.org/10.1007/978-3-031-10989-8_52,False,False,True,,"International Conference on Knowledge Science, Engineering and Management",DEADBEEF
2021,ACM,article,__no_data__,['BERT'],__no_data__,HMC-TRAN: A Tensor-core Inspired Hierarchical Model Compression for Transformer-based DNNs on GPU,https://doi.org/10.1145/3453688.3461740,https://doi.org/10.1145/3453688.3461740,False,False,True,,Great Lakes Symposium on VLSI,DEADBEEF
2021,None,article,__no_data__,['BERT'],__no_data__,HoloFormer: Deep Compression of Pre-Trained Transforms via Unified Optimization of N: M Sparsity and Integer Quantization,None,None,https://openreview.net/pdf?id=eAEcdRkcMHh,check,check,,None,DEADBEEF
2021,Springer,article,__no_data__,['BERT'],__no_data__,How Deep Learning Model Architecture and Software Stack Impacts Training Performance in the Cloud,https://doi.org/978--3--030--89385--9,https://doi.org/978-3-030-89385-9,False,check,False,,Engineering Artificially Intelligent Systems,DEADBEEF
2021,arXiv,article,__no_data__,['BERT'],__no_data__,How to Train BERT with an Academic Budget,https://doi.org/10.48550/arXiv.2104.07705,https://doi.org/10.48550/arXiv.2104.07705,https://arxiv.org/pdf/2104.07705.pdf,True,False,,Computer Science > Computation and Language,DEADBEEF
2021,PMLR,article,__no_data__,['BERT'],__no_data__,I-BERT: Integer-only BERT Quantization,https://doi.org/10.48550/arXiv.2101.01321,https://proceedings.mlr.press/v139/kim21d.html,http://proceedings.mlr.press/v139/kim21d/kim21d.pdf,False,True,,Proceedings of Machine Learning Research,DEADBEEF
2020,Springer,article,__no_data__,['BERT'],__no_data__,Improving Accuracy and Speeding Up Document Image Classification Through Parallel Systems,https://doi.org/10.1007/978--3--030--50417--5_29,https://doi.org/10.1007/978-3-030-50417-5_29,https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7302855/pdf/978-3-030-50417-5_Chapter_29.pdf,False,True,,International Conference on Computational Science,DEADBEEF
2022,Springer,article,__no_data__,['BERT'],__no_data__,Improving Oversubscribed GPU Memory Performance in the PyTorch Framework,https://doi.org/10.1007/s10586--022--03805--x,https://doi.org/10.1007/s10586-022-03805-x,False,False,True,,Cluster Computing,DEADBEEF
2020,arXiv,article,__no_data__,['BERT'],__no_data__,Improving post training neural quantization: Layer-wise calibration and integer programming,https://arxiv.org/abs/2006.10518,https://arxiv.org/abs/2006.10518,https://arxiv.org/pdf/2006.10518.pdf,False,True,,Computer Science > Machine Learning,DEADBEEF
2021,IEEE,article,__no_data__,['BERT'],__no_data__,Improving the efficiency of transformers for resource-constrained devices,https://doi.org/10.1109/DSD53832.2021.00074,https://doi.org/10.1109/DSD53832.2021.00074,https://arxiv.org/pdf/2106.16006.pdf,False,True,,Euromicro Conference on Digital System Design (DSD),DEADBEEF
2022,arXiv,article,__no_data__,['BERT'],__no_data__,Integer Fine-tuning of Transformer-based Models,https://doi.org/10.48550/arXiv.2209.09815,https://doi.org/10.48550/arXiv.2209.09815,https://arxiv.org/pdf/2209.09815.pdf,check,False,,Computer Science > Machine Learning,DEADBEEF
2020,arXiv,article,__no_data__,['BERT'],__no_data__,Integer quantization for deep learning inference: Principles and empirical evaluation,https://doi.org/10.48550/arXiv.2004.09602,https://doi.org/10.48550/arXiv.2004.09602,https://arxiv.org/pdf/2004.09602.pdf,False,True,,Computer Science > Machine Learning,DEADBEEF
2021,ACM,article,__no_data__,['BERT'],__no_data__,KAISA: An adaptive second-order optimizer framework for deep neural networks,https://doi.org/10.1145/3458817.3476152,https://doi.org/10.1145/3458817.3476152,https://arxiv.org/pdf/2107.01739.pdf,False,check,,"International Conference for High Performance Computing, Networking, Storage and Analysis",DEADBEEF
2021,arXiv,article,__no_data__,['BERT'],__no_data__,KDLSQ-BERT: A Quantized Bert Combining Knowledge Distillation with Learned Step Size Quantization,https://doi.org/10.48550/arXiv.2101.05938,https://doi.org/10.48550/arXiv.2101.05938,https://arxiv.org/pdf/2101.05938.pdf,check,check,,Computer Science > Computation and Language,DEADBEEF
2021,IEEE,article,__no_data__,['BERT'],__no_data__,Kunlun: A 14nm High-Performance AI Processor for Diversified Workloads,https://doi.org/10.1109/ISSCC42613.2021.9366056,https://doi.org/10.1109/ISSCC42613.2021.9366056,False,False,True,,IEEE International Solid- State Circuits Conference (ISSCC),DEADBEEF
2020,arXiv,article,__no_data__,['BERT'],__no_data__,Ladabert: Lightweight adaptation of bert through hybrid model compression,https://doi.org/10.48550/arXiv.2004.04124,https://doi.org/10.48550/arXiv.2004.04124,https://arxiv.org/pdf/2004.04124.pdf,check,check,,Computer Science > Computation and Language,DEADBEEF
2021,IEEE,article,__no_data__,['BERT'],__no_data__,Layerweaver: Maximizing Resource Utilization of Neural Processing Units via Layer-Wise Scheduling,https://doi.org/10.1109/HPCA51647.2021.00056,https://doi.org/10.1109/HPCA51647.2021.00056,https://taejunham.github.io/data/layerweaver_hpca21.pdf,False,True,,International Symposium on High-Performance Computer Architecture (HPCA),DEADBEEF
2022,ACM,article,__no_data__,['BERT'],__no_data__,Learned Token Pruning in Contextualized Late Interaction over BERT (ColBERT),https://doi.org/10.1145/3477495.3531835,https://doi.org/10.1145/3477495.3531835,https://web.Arxiv.org/web/20220713100651id_/https://dl.acm.org/doi/pdf/10.1145/3477495.3531835,True,False,,ACM SIGIR Conference on Research and Development in Information Retrieval,DEADBEEF
2021,AAAI,article,__no_data__,['BERT'],__no_data__,Learning Light-Weight Translation Models from Deep Transformer,https://doi.org/10.1609/aaai.v35i15.17561,https://doi.org/10.1609/aaai.v35i15.17561,https://ojs.aaai.org/index.php/AAAI/article/view/17561/17368,check,False,,AAAI Technical Track on Speech and Natural Language Processing II,DEADBEEF
2022,ACM,article,__no_data__,['BERT'],__no_data__,Lightweight Composite Re-Ranking for Efficient Keyword Search with BERT,https://doi.org/10.1145/3488560.3498495,https://doi.org/10.1145/3488560.3498495,https://dl.acm.org/doi/pdf/10.1145/3488560.3498495,check,False,,ACM International Conference on Web Search and Data Mining,DEADBEEF
2022,ACL,article,__no_data__,['BERT'],__no_data__,Lightweight Transformers for Conversational AI,http://dx.doi.org/10.18653/v1/2022.naacl--industry.25,http://dx.doi.org/10.18653/v1/2022.naacl-industry.25,https://aclanthology.org/2022.naacl-industry.25.pdf,check,check,,Conference of the North American Chapter of the Association for Computational Linguistics,DEADBEEF
2022,arXiv,article,__no_data__,['BERT'],__no_data__,LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale,https://doi.org/10.48550/arXiv.2208.07339,https://doi.org/10.48550/arXiv.2208.07339,https://arxiv.org/pdf/2208.07339,False,True,,Computer Science > Machine Learning,DEADBEEF
2020,arXiv,article,__no_data__,['BERT'],__no_data__,Load What You Need: Smaller Versions of Multilingual BERT,https://doi.org/10.48550/arXiv.2010.05609,https://doi.org/10.48550/arXiv.2010.05609,https://arxiv.org/pdf/2010.05609.pdf,check,check,,Computer Science > Computation and Language,DEADBEEF
2020,IEEE/ACM,article,__no_data__,['BERT'],__no_data__,Look-Up Table based Energy Efficient Processing in Cache Support for Neural Network Acceleration,https://doi.org/10.1109/MICRO50266.2020.00020,https://doi.org/10.1109/MICRO50266.2020.00020,https://www.microarch.org/micro53/papers/738300a088.pdf,False,True,,IEEE/ACM International Symposium on Microarchitecture (MICRO),DEADBEEF
2022,Springer,article,__no_data__,['BERT'],__no_data__,Low-Bit Quantization of Transformer for Audio Speech Recognition,https://doi.org/10.1007/978--3--031--19032--2_12,https://doi.org/10.1007/978-3-031-19032-2_12,False,check,check,,None,DEADBEEF
2022,IEEE,article,__no_data__,['BERT'],__no_data__,Low-Precision Quantization Techniques for Hardware-Implementation-Friendly BERT Models,https://doi.org/10.1109/ISQED54688.2022.9806238,https://doi.org/10.1109/ISQED54688.2022.9806238,False,False,True,,International Symposium on Quality Electronic Design (ISQED),DEADBEEF
2021,Springer,article,__no_data__,['BERT'],__no_data__,M2M: Learning to Enhance Low-Light Image from Model to Mobile FPGA,https://doi.org/10.1007/978--3--030--89029--2_22,https://doi.org/10.1007/978-3-030-89029-2_22,False,False,True,,Computer Graphics International Conference,DEADBEEF
2019,IEEE,article,__no_data__,['BERT'],__no_data__,MAGNet: A Modular Accelerator Generator for Neural Networks,https://doi.org/10.1109/ICCAD45719.2019.8942127,https://doi.org/10.1109/ICCAD45719.2019.8942127,https://people.eecs.berkeley.edu/~ysshao/assets/papers/magnet2019-iccad.pdf,check,check,,IEEE/ACM International Conference on Computer-Aided Design (ICCAD),DEADBEEF
2020,ACM,article,__no_data__,['BERT'],__no_data__,MiniLM: Deep Self-Attention Distillation for Task-Agnostic Compression of Pre-Trained Transformers,https://dl.acm.org/doi/abs/10.5555/3495724.3496209,https://dl.acm.org/doi/abs/10.5555/3495724.3496209,https://dl.acm.org/doi/pdf/10.5555/3495724.3496209,check,check,,International Conference on Neural Information Processing Systems,DEADBEEF
2022,arXiv,article,__no_data__,['BERT'],__no_data__,MKQ-BERT: Quantized BERT with 4-bits Weights and Activations,https://doi.org/10.48550/arXiv.2203.13483,https://doi.org/10.48550/arXiv.2203.13483,https://arxiv.org/pdf/2203.13483.pdf,False,True,,Computer Science > Machine Learning,DEADBEEF
2022,IEEE,article,__no_data__,['BERT'],__no_data__,Mokey: enabling narrow fixed-point inference for out-of-the-box floating-point transformer models,https://doi.org/10.1145/3470496.3527438,https://doi.org/10.1145/3470496.3527438,https://arxiv.org/pdf/2203.12758.pdf,False,True,,International Symposium on Computer Architecture,DEADBEEF
2020,NeurIPS,article,__no_data__,['BERT'],__no_data__,Movement Pruning: Adaptive Sparsity by Fine-Tuning,https://doi.org/10.48550/arXiv.2005.07683,https://doi.org/10.48550/arXiv.2005.07683,https://proceedings.neurips.cc/paper/2020/file/eae15aabaa768ae4a5993a8a4f4fa6e4-Paper.pdf,check,False,,Advances in Neural Information Processing Systems,DEADBEEF
2022,IEEE/CVF,article,__no_data__,['BERT'],__no_data__,Mr. BiQ: Post-Training Non-Uniform Quantization Based on Minimizing the Reconstruction Error,https://doi.org/10.1109/CVPR52688.2022.01201,https://doi.org/10.1109/CVPR52688.2022.01201,https://openaccess.thecvf.com/content/CVPR2022/papers/Jeon_Mr.BiQ_Post-Training_Non-Uniform_Quantization_Based_on_Minimizing_the_Reconstruction_Error_CVPR_2022_paper.pdf,False,check,CVPR,IEEE/CVF Conference on Computer Vision and Pattern Recognition,DEADBEEF
2019,IEEE,article,__no_data__,['BERT'],__no_data__,mRNA: Enabling Efficient Mapping Space Exploration for a Reconfiguration Neural Accelerator,https://doi.org/10.1109/ISPASS.2019.00040,https://doi.org/10.1109/ISPASS.2019.00040,https://bpb-us-w2.wpmucdn.com/sites.gatech.edu/dist/c/332/files/2019/02/mrna_ispass2019.pdf,check,True,,IEEE International Symposium on Performance Analysis of Systems and Software (ISPASS),DEADBEEF
2020,arXiv,article,__no_data__,['BERT'],__no_data__,"MSP: an FPGA-specific mixed-scheme, multi-precision deep neural network quantization framework",https://doi.org/10.48550/arXiv.2009.07460,https://doi.org/10.48550/arXiv.2009.07460,https://arxiv.org/pdf/2009.07460.pdf,check,True,,Computer Science > Machine Learning,DEADBEEF
2021,ACM,article,__no_data__,['BERT'],__no_data__,NAS-BERT: Task-Agnostic and Adaptive-Size BERT Compression with Neural Architecture Search,https://doi.org/10.1145/3447548.3467262,https://doi.org/10.1145/3447548.3467262,https://arxiv.org/pdf/2105.14444.pdf,check,check,,ACM SIGKDD Conference on Knowledge Discovery & Data Mining,DEADBEEF
2022,ACM,article,__no_data__,['BERT'],__no_data__,Near-Optimal Sparse Allreduce for Distributed Deep Learning,https://doi.org/10.1145/3503221.3508399,https://doi.org/10.1145/3503221.3508399,https://arxiv.org/pdf/2201.07598.pdf,False,True,,ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming,DEADBEEF
2022,MDPI,article,__no_data__,['BERT'],__no_data__,Nebula: A Scalable and Flexible Accelerator for DNN Multi-Branch Blocks on Embedded Systems,https://doi.org/10.3390/electronics11040505,https://doi.org/10.3390/electronics11040505,https://www.mdpi.com/2079-9292/11/4/505/pdf,True,True,,Electronics,DEADBEEF
2022,Wiley,article,__no_data__,['BERT'],__no_data__,NEEBS: Nonexpert large-scale environment building system for deep neural network,https://doi.org/10.1002/cpe.7499,https://doi.org/10.1002/cpe.7499,False,check,False,,Concurrency and Computation Practice and Experience,DEADBEEF
2021,CARRV,article,__no_data__,['BERT'],__no_data__,NeuralScale: A RISC-V Based Neural Processor Boosting AI Inference in Clouds,https://carrv.github.io/2021/,https://carrv.github.io/2021/,https://carrv.github.io/2021/papers/CARRV2021_paper_67_Zhan.pdf,False,True,,Computer Architecture Research with RISC-V,DEADBEEF
2021,IEEE,article,__no_data__,['BERT'],__no_data__,"NLP-Fast: A Fast, Scalable, and Flexible System to Accelerate Large-Scale Heterogeneous NLP Models",https://doi.org/10.1109/PACT52795.2021.00013,https://doi.org/10.1109/PACT52795.2021.00013,False,False,True,,International Conference on Parallel Architectures and Compilation Techniques (PACT),DEADBEEF
2021,ACM/SIGDA,article,__no_data__,['BERT'],__no_data__,NPE: An FPGA-based Overlay Processor for Natural Language Processing,https://doi.org/10.1145/3431920.3439477,https://doi.org/10.1145/3431920.3439477,https://arxiv.org/pdf/2104.06535.pdf,False,True,,ACM/SIGDA International Symposium on Field-Programmable Gate Arrays,DEADBEEF
2022,NeurIPS,article,__no_data__,['BERT'],__no_data__,Optimal Brain Compression: A framework for accurate post-training quantization and pruning,https://doi.org/10.48550/arXiv.2208.11580,https://doi.org/10.48550/arXiv.2208.11580,https://arxiv.org/pdf/2208.11580.pdf,False,False,,Conference on Neural Information Processing Systems,DEADBEEF
2022,Springer,article,__no_data__,['BERT'],__no_data__,PipeBERT: High-throughput BERT Inference for ARM Big.LITTLE Multi-core Processors,https://doi.org/10.1007/s11265--022--01814--y,https://doi.org/10.1007/s11265-022-01814-y,False,False,True,,Journal of Signal Processing Systems,DEADBEEF
2020,Arxiv,article,__no_data__,['BERT'],__no_data__,Poor Man's BERT: Smaller and Faster Transformer Models,https://doi.org/10.48550/arXiv.2004.03844,https://doi.org/10.48550/arXiv.2004.03844,https://arxiv.org/pdf/2004.03844v1,False,False,,Computer Science > Computation and Language,DEADBEEF
2022,IEEE,article,__no_data__,['BERT'],__no_data__,Post-Training Quantization for Longformer with Chunkwise Quantization Granularity and Optimized Percentile,https://doi.org/10.1109/ICCCS55155.2022.9846198,https://doi.org/10.1109/ICCCS55155.2022.9846198,False,False,True,,International Conference on Computer and Communication Systems (ICCCS),DEADBEEF
2020,PMLR,article,__no_data__,['BERT'],__no_data__,PoWER-BERT: Accelerating BERT Inference via Progressive Word-vector Elimination,https://proceedings.mlr.press/v119/goyal20a.html,https://proceedings.mlr.press/v119/goyal20a.html,http://proceedings.mlr.press/v119/goyal20a/goyal20a.pdf,False,False,,Proceedings of Machine Learning Research,DEADBEEF
2019,ACM,article,__no_data__,['BERT'],__no_data__,Pre-trained bert-gru model for relation extraction,https://doi.org/10.1145/3373509.3373533,https://doi.org/10.1145/3373509.3373533,False,False,False,,International Conference on Computing and Pattern Recognition,DEADBEEF
2022,Springer,article,__no_data__,['BERT'],__no_data__,Pre-trained Language Model with Feature Reduction and No Fine-Tuning,https://doi.org/10.1007/978--981--19--3923--5_59,https://doi.org/10.1007/978-981-19-3923-5_59,False,False,False,,"Control, Instrumentation and Mechatronics: Theory and Practice",DEADBEEF
2021,ACM,article,__no_data__,['BERT'],__no_data__,Predicting Efficiency/Effectiveness Trade-offs for Dense vs. Sparse Retrieval Strategy Selection,https://doi.org/10.1145/3459637.3482159,https://doi.org/10.1145/3459637.3482159,False,False,False,,ACM International Conference on Information & Knowledge Management,DEADBEEF
2022,Arxiv,article,__no_data__,['BERT'],__no_data__,Privacy-Preserving Text Classification on BERT Embeddings with Homomorphic Encryption,https://arxiv.org/abs/2210.02574,https://arxiv.org/abs/2210.02574,https://arxiv.org/pdf/2210.02574.pdf,False,True,,Computation and Language,DEADBEEF
2022,ACM,article,__no_data__,['BERT'],__no_data__,ProSE: the architecture and design of a protein discovery engine,https://doi.org/10.1145/3503222.3507722,https://doi.org/10.1145/3503222.3507722,https://par.nsf.gov/servlets/purl/10394954,False,True,,ACM International Conference on Architectural Support for Programming Languages and Operating Systems,DEADBEEF
2020,Arxiv,article,__no_data__,['BERT'],__no_data__,Pruning Redundant Mappings in Transformer Models via Spectral-Normalized Identity Prior,https://arxiv.org/abs/2010.01791,https://arxiv.org/abs/2010.01791,https://arxiv.org/pdf/2010.01791.pdf,False,check,,Computer Science > Computation and Language,DEADBEEF
2021,Arxiv,article,__no_data__,['BERT'],__no_data__,PTQ4ViT: Post-Training Quantization Framework for Vision Transformers with Twin Uniform Quantization,https://arxiv.org/abs/2111.12293,https://arxiv.org/abs/2111.12293,https://arxiv.org/pdf/2111.12293,False,True,,Computer Science > Computer Vision and Pattern Recognition,DEADBEEF
2020,AAAI,article,__no_data__,['BERT'],__no_data__,Q-BERT: Hessian Based Ultra Low Precision Quantization of BERT,https://doi.org/10.1609/aaai.v34i05.6409,https://doi.org/10.1609/aaai.v34i05.6409,https://ojs.aaai.org/index.php/AAAI/article/view/6409/6265,False,check,,AAAI Technical Track: Natural Language Processing,DEADBEEF
2019,IEEE,article,__no_data__,['BERT'],__no_data__,Q8BERT: Quantized 8Bit BERT,https://doi.org/10.1109/EMC2--NIPS53020.2019.00016,https://doi.org/10.1109/EMC2-NIPS53020.2019.00016,https://arxiv.org/pdf/1910.06188.pdf,False,False,,Workshop on Energy Efficient Machine Learning and Cognitive Computing - NeurIPS Edition (EMC2-NIPS),DEADBEEF
2022,Arxiv,article,__no_data__,['BERT'],__no_data__,QDrop: Randomly Dropping Quantization for Extremely Low-bit Post-Training Quantization,https://arxiv.org/abs/2203.05740,https://arxiv.org/abs/2203.05740,https://arxiv.org/pdf/2203.05740,False,False,,Computer Science > Computer Vision and Pattern Recognition,DEADBEEF
2022,Arxiv,article,__no_data__,['BERT'],__no_data__,QuaLA-MiniLM: a Quantized Length Adaptive MiniLM,https://arxiv.org/abs/2210.17114,https://arxiv.org/abs/2210.17114,https://arxiv.org/pdf/2210.17114,False,False,,Computer Science > Computation and Language,DEADBEEF
2021,IEEE,article,__no_data__,['BERT'],__no_data__,Randomly Wired Network Based on RoBERTa and Dialog History Attention for Response Selection,https://doi.org/10.1109/TASLP.2021.3077119,https://doi.org/10.1109/TASLP.2021.3077119,False,False,False,,"IEEE/ACM Transactions on Audio, Speech, and Language Processing",DEADBEEF
2022,IEEE,article,__no_data__,['BERT'],__no_data__,RCT: Resource Constrained Training for Edge AI,https://doi.org/10.1109/TNNLS.2022.3190451,https://doi.org/10.1109/TNNLS.2022.3190451,https://arxiv.org/pdf/2103.14493.pdf,False,check,,IEEE Transactions on Neural Networks and Learning Systems,DEADBEEF
2021,ACM,article,__no_data__,['BERT'],__no_data__,Re2PIM: A Reconfigurable ReRAM-Based PIM Design for Variable-Sized Vector-Matrix Multiplication,https://doi.org/10.1145/3453688.3461494,https://doi.org/10.1145/3453688.3461494,False,False,True,,Proceedings on Great Lakes Symposium on VLSI,DEADBEEF
2022,IEEE,article,__no_data__,['BERT'],__no_data__,ReAAP: A Reconfigurable and Algorithm-Oriented Array Processor With Compiler-Architecture Co-Design,https://doi.org/10.1109/TC.2022.3213177,https://doi.org/10.1109/TC.2022.3213177,https://ieeexplore.ieee.org/iel7/12/4358213/09914609.pdf,False,True,,IEEE Transactions on Computers,DEADBEEF
2020,ACM,article,__no_data__,['BERT'],__no_data__,ReTransformer: ReRAM-based processing-in-memory architecture for transformer acceleration,https://doi.org/10.1145/3400302.3415640,https://doi.org/10.1145/3400302.3415640,https://dl.acm.org/doi/pdf/10.1145/3400302.3415640,False,True,,International Conference on Computer-Aided Design,DEADBEEF
2021,IEEE,article,__no_data__,['BERT'],__no_data__,RISC-VTF: RISC-V Based Extended Instruction Set for Transformer,https://doi.org/10.1109/SMC52423.2021.9658643,https://doi.org/10.1109/SMC52423.2021.9658643,False,False,True,,"IEEE International Conference on Systems, Man, and Cybernetics",DEADBEEF
2021,None,article,__no_data__,['BERT'],__no_data__,RMSMP: A Novel Deep Neural Network Quantization Framework with Row-wise Mixed Schemes and Multiple Precisions,xx,None,https://openaccess.thecvf.com/content/ICCV2021/papers/Chang_RMSMP_A_Novel_Deep_Neural_Network_Quantization_Framework_With_Row-Wise_ICCV_2021_paper.pdf,False,True,,IEEE/CVF International Conference on Computer Vision,DEADBEEF
2022,IEEE,article,__no_data__,['BERT'],__no_data__,Row-wise Accelerator for Vision Transformer,https://doi.org/10.1109/AICAS54282.2022.9869928,https://doi.org/10.1109/AICAS54282.2022.9869928,https://arxiv.org/pdf/2205.03998.pdf,False,True,,International Conference on Artificial Intelligence Circuits and Systems,DEADBEEF
2022,Arxive,article,__no_data__,['BERT'],__no_data__,"S4: a High-sparsity, High-performance AI Accelerator",https://arxiv.org/abs/2207.08006,https://arxiv.org/abs/2207.08006,https://arxiv.org/pdf/2207.08006,False,True,,Computer Science > Hardware Architecture,DEADBEEF
2022,ACM,article,__no_data__,['BERT'],__no_data__,SALO: an efficient spatial accelerator enabling hybrid sparse attention mechanisms for long sequences,https://doi.org/10.1145/3489517.3530504,https://doi.org/10.1145/3489517.3530504,https://arxiv.org/pdf/2206.14550.pdf,False,True,,ACM/IEEE Design Automation Conference,DEADBEEF
2021,IEEE/ACM,article,__no_data__,['BERT'],__no_data__,Sanger: A Co-Design Framework for Enabling Sparse Attention using Reconfigurable Architecture,https://doi.org/10.1145/3466752.3480125,https://doi.org/10.1145/3466752.3480125,https://dl.acm.org/doi/pdf/10.1145/3466752.3480125,False,True,,IEEE/ACM International Symposium on Microarchitecture,DEADBEEF
2022,None,article,__no_data__,['BERT'],__no_data__,Searching for memory-lighter architectures for OCR-augmented image captioning,https://doi.org/10.3233/JIFS--219230,https://doi.org/10.3233/JIFS-219230,False,False,check,,Journal of Intelligent & Fuzzy Systems,DEADBEEF
2023,Elsevier,article,__no_data__,['BERT'],__no_data__,SECDA-TFLite: A toolkit for efficient development of FPGA-based DNN accelerators for edge inference,https://doi.org/10.1016/j.jpdc.2022.11.005,https://doi.org/10.1016/j.jpdc.2022.11.005,https://www.sciencedirect.com/science/article/pii/S0743731522002301/pdfft?md5=444fdc7e73724f5d9881d162bed2a735&pid=1-s2.0-S0743731522002301-main.pdf,False,True,,Journal of Parallel and Distributed Computing,DEADBEEF
2022,PLOSONE,article,__no_data__,['BERT'],__no_data__,SensiMix: Sensitivity-Aware 8-bit index & 1-bit value mixed precision quantization for BERT compression,https://doi.org/10.1371/journal.pone.0265621,https://doi.org/10.1371/journal.pone.0265621,https://journals.plos.org/plosone/article/file?id=10.1371/journal.pone.0265621&type=printable,False,check,,None,DEADBEEF
2022,IEEE,article,__no_data__,['BERT'],__no_data__,Sentiment Analysis Using Pre-Trained Language Model With No Fine-Tuning and Less Resource,https://doi.org/10.1109/ACCESS.2022.3212367,https://doi.org/10.1109/ACCESS.2022.3212367,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9912410,False,True,,IEEE Access,DEADBEEF
2021,Springer,article,__no_data__,['BERT'],__no_data__,Simplified TinyBERT: Knowledge Distillation for Document Retrieval,https://doi.org/10.1007/978--3--030--72240--1_21,https://doi.org/10.1007/978-3-030-72240-1_21,https://arxiv.org/pdf/2009.07531.pdf,False,check,,Advances in Information Retrieval,DEADBEEF
2021,IEEE,article,__no_data__,['BERT'],__no_data__,SmaQ: Smart Quantization for DNN Training by Exploiting Value Clustering,https://doi.org/10.1109/LCA.2021.3108505,https://doi.org/10.1109/LCA.2021.3108505,https://hparch.gatech.edu/papers/nima_2021_cal.pdf,False,True,,IEEE Computer Architecture Letters,DEADBEEF
2023,Arxiv,article,__no_data__,['BERT'],__no_data__,SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models,https://arxiv.org/abs/2211.10438,https://arxiv.org/abs/2211.10438,https://arxiv.org/pdf/2211.10438.pdf,False,True,,Computer Science > Computation and Language',DEADBEEF
2021,ACM/IEEE,article,__no_data__,['BERT'],__no_data__,Softermax: Hardware/Software Co-Design of an Efficient Softmax for Transformers,https://doi.org/10.1109/DAC18074.2021.9586134,https://doi.org/10.1109/DAC18074.2021.9586134,https://arxiv.org/pdf/2103.09301.pdf,False,True,,ACM/IEEE Design Automation Conference,DEADBEEF
2022,Springer,article,__no_data__,['BERT'],__no_data__,Software and Hardware Fusion Multi-Head Attention,http://dx.doi.org/10.1007/978--3--031--10989--8_51,http://dx.doi.org/10.1007/978-3-031-10989-8_51,False,False,True,,"International Conference on Knowledge Science, Engineering and Management",DEADBEEF
2022,IEEE,article,__no_data__,['BERT'],__no_data__,Sparse Attention Acceleration with Synergistic In-Memory Pruning and On-Chip Recomputation,https://doi.org/10.1109/MICRO56248.2022.00059,https://doi.org/10.1109/MICRO56248.2022.00059,https://arxiv.org/pdf/2209.00606.pdf,False,True,,IEEE/ACM International Symposium on Microarchitecture,DEADBEEF
2023,Arxiv,article,__no_data__,['BERT'],__no_data__,Sparse*BERT: Sparse Models Generalize To New tasks and Domains,https://arxiv.org/abs/2205.12452,https://arxiv.org/abs/2205.12452,https://arxiv.org/pdf/2205.12452,False,check,,Computer Science > Computation and Language,DEADBEEF
2018,None,article,__no_data__,['BERT'],__no_data__,SparseNN: An energy-efficient neural network accelerator exploiting input and output sparsity,https://doi.org/10.23919/DATE.2018.8342010,https://doi.org/10.23919/DATE.2018.8342010,https://doi.org/10.23919/DATE.2018.8342010,True,True,,"Design, Automation & Test in Europe Conference & Exhibition",DEADBEEF
2021,IEEE,article,__no_data__,['BERT'],__no_data__,SpAtten: Efficient Sparse Attention Architecture with Cascade Token and Head Pruning,https://doi.org/10.1109/HPCA51647.2021.00018,https://doi.org/10.1109/HPCA51647.2021.00018,https://arxiv.org/pdf/2012.09852.pdf,False,True,,International Symposium on High-Performance Computer Architecture,DEADBEEF
2021,Arxiv,article,__no_data__,['BERT'],__no_data__,SQuAT: Sharpness- and Quantization-Aware Training for BERT,https://arxiv.org/abs/2210.07171,https://arxiv.org/abs/2210.07171,https://arxiv.org/pdf/2210.07171.pdf,False,True,,Computer Science > Machine Learning,DEADBEEF
2020,Arxiv,article,__no_data__,['BERT'],__no_data__,SqueezeBERT: What can computer vision teach NLP about efficient neural networks?,https://arxiv.org/abs/2006.11316,https://arxiv.org/abs/2006.11316,https://arxiv.org/pdf/2006.11316.pdf,False,check,,Computer Science > Computation and Language,DEADBEEF
2021,AAAI,article,__no_data__,['BERT'],__no_data__,Stochastic precision ensemble: self-knowledge distillation for quantized deep neural networks,https://doi.org/10.1609/aaai.v35i8.16839,https://doi.org/10.1609/aaai.v35i8.16839,https://ojs.aaai.org/index.php/AAAI/article/view/16839/16646,False,False,,AAAI Technical Track on Machine Learning I,DEADBEEF
2019,Arxiv,article,__no_data__,['BERT'],__no_data__,Structured pruning of a BERT-based question answering model,https://arxiv.org/abs/1910.06360,https://arxiv.org/abs/1910.06360,https://arxiv.org/pdf/1910.06360,False,False,,Computer Science > Computation and Language,DEADBEEF
2019,Arxiv,article,__no_data__,['BERT'],__no_data__,Structured pruning of large language models,https://arxiv.org/abs/1910.04732,https://arxiv.org/abs/1910.04732,https://arxiv.org/pdf/1910.04732,False,False,,Computer Science > Computation and Language,DEADBEEF
2022,ACM,article,__no_data__,['BERT'],__no_data__,SwiftPruner: Reinforced Evolutionary Pruning for Efficient Ad Relevance,https://doi.org/10.1145/3511808.3557139,https://doi.org/10.1145/3511808.3557139,https://arxiv.org/pdf/2209.00625.pdf,False,False,,ACM International Conference on Information & Knowledge Management,DEADBEEF
2022,UCLA,article,__no_data__,['BERT'],__no_data__,T-OPU: An FPGA-based Overlay Processor for Natural Language Processing,https://escholarship.org/uc/item/9r46v693,https://escholarship.org/uc/item/9r46v693,https://escholarship.org/content/qt9r46v693/qt9r46v693.pdf,False,True,,Open Access Publications from the University of California,DEADBEEF
2021,IEEE,article,__no_data__,['BERT'],__no_data__,Talos: A Weighted Speedup-Aware Device Placement of Deep Learning Models,https://doi.org/10.1109/ASAP52443.2021.00023,https://doi.org/10.1109/ASAP52443.2021.00023,False,False,True,,"International Conference on Application-specific Systems, Architectures and Processors",DEADBEEF
2023,Arxiv,article,__no_data__,['BERT'],__no_data__,Teacher Intervention: Improving Convergence of Quantization Aware Training for Ultra-Low Precision Transformers,https://arxiv.org/abs/2302.11812,https://arxiv.org/abs/2302.11812,https://arxiv.org/pdf/2302.11812,False,True,,Computer Science > Computation and Language,DEADBEEF
2020,Arxiv,article,__no_data__,['BERT'],__no_data__,TernaryBERT: Distillation-aware Ultra-low Bit BERT,https://arxiv.org/abs/2009.12812,https://arxiv.org/abs/2009.12812,https://arxiv.org/pdf/2009.12812,False,check,,Computer Science > Computation and Language,DEADBEEF
2022,Arxiv,article,__no_data__,['BERT'],__no_data__,The Optimal BERT Surgeon: Scalable and Accurate Second-Order Pruning for Large Language Models,https://arxiv.org/abs/2203.07259,https://arxiv.org/abs/2203.07259,https://arxiv.org/pdf/2203.07259.pdf,False,check,,Computer Science > Computation and Language,DEADBEEF
2023,ACM,article,__no_data__,['BERT'],__no_data__,TiC-SAT: Tightly-Coupled Systolic Accelerator for Transformers,https://doi.org/10.1145/3566097.3567867,https://doi.org/10.1145/3566097.3567867,https://infoscience.epfl.ch/record/298067/files/TiC_SAT_ASPDAC-preprint.pdf,False,True,,Asia and South Pacific Design Automation Conference,DEADBEEF
2019,Arxiv,article,__no_data__,['BERT'],__no_data__,Tinybert: Distilling bert for natural language understanding,https://arxiv.org/abs/1909.10351,https://arxiv.org/abs/1909.10351,https://arxiv.org/pdf/1909.10351,False,check,,Computer Science > Computation and Language,DEADBEEF
2022,NeurIPS,article,__no_data__,['BERT'],__no_data__,Towards efficient post-training quantization of pre-trained language models,link,https://proceedings.neurips.cc/paper_files/paper/2022/hash/096347b4efc264ae7f07742fea34af1f-Abstract-Conference.html,https://proceedings.neurips.cc/paper_files/paper/2022/file/096347b4efc264ae7f07742fea34af1f-Paper-Conference.pdf,False,False,,Advances in Neural Information Processing Systems,DEADBEEF
2021,Arxiv,article,__no_data__,['BERT'],__no_data__,TR-BERT: Dynamic Token Reduction for Accelerating BERT Inference,https://arxiv.org/abs/2105.11618,https://arxiv.org/abs/2105.11618,https://arxiv.org/pdf/2105.11618.pdf,False,True,,Computer Science > Computation and Language,DEADBEEF
2022,Arxiv,article,__no_data__,['BERT'],__no_data__,"Train Flat, Then Compress: Sharpness-Aware Minimization Learns More Compressible Models",https://arxiv.org/abs/2205.12694,https://arxiv.org/abs/2205.12694,https://arxiv.org/pdf/2205.12694.pdf,False,True,,Computer Science > Computation and Language,DEADBEEF
2020,Arxiv,article,__no_data__,['BERT'],__no_data__,Training Large Neural Networks with Constant Memory using a New Execution Algorithm,https://arxiv.org/abs/2002.05645,https://arxiv.org/abs/2002.05645,https://arxiv.org/pdf/2002.05645.pdf,False,True,,Computer Science > Machine Learning,DEADBEEF
2021,Arxiv,article,__no_data__,['BERT'],__no_data__,Training with Quantization Noise for Extreme Model Compression,https://arxiv.org/abs/2004.07320,https://arxiv.org/abs/2004.07320,https://arxiv.org/pdf/2004.07320.pdf,False,True,,Computer Science > Machine Learning,DEADBEEF
2022,IEEE,article,__no_data__,['BERT'],__no_data__,TranCIM: Full-Digital Bitline-Transpose CIM-based Sparse Transformer Accelerator With Pipeline/Parallel Reconfigurable Modes,https://doi.org/10.1109/JSSC.2022.3213542,https://doi.org/10.1109/JSSC.2022.3213542,False,False,True,,IEEE Journal of Solid-State Circuits,DEADBEEF
2021,Arxiv,article,__no_data__,['BERT'],__no_data__,Transformer Acceleration with Dynamic Sparse Attention,https://arxiv.org/abs/2110.11299,https://arxiv.org/abs/2110.11299,https://arxiv.org/pdf/2110.11299,False,True,,Computer Science > Machine Learning,DEADBEEF
2022,IEEE,article,__no_data__,['BERT'],__no_data__,TransPIM: A Memory-based Acceleration via Software-Hardware Co-Design for Transformer,https://doi.org/10.1109/HPCA53966.2022.00082,https://doi.org/10.1109/HPCA53966.2022.00082,https://par.nsf.gov/servlets/purl/10345536,False,True,,IEEE International Symposium on High-Performance Computer Architecture (HPCA),DEADBEEF
2020,IEEE,article,__no_data__,['BERT'],__no_data__,"Ultron-AutoML: An open-source, distributed, scalable framework for efficient hyper-parameter optimization",https://doi.org/10.1109/BigData50022.2020.9378071,https://doi.org/10.1109/BigData50022.2020.9378071,https://ashish-gupta03.github.io/files/Ultron.pdf,False,True,,International Conference on Big Data (Big Data),DEADBEEF
2021,Arxiv,article,__no_data__,['BERT'],__no_data__,Understanding and Overcoming the Challenges of Efficient Transformer Quantization,https://arxiv.org/abs/2109.12948,https://arxiv.org/abs/2109.12948,https://arxiv.org/pdf/2109.12948.pdf,False,True,,Computer Science > Machine Learning,DEADBEEF
2022,Arxiv,article,__no_data__,['BERT'],__no_data__,VAQF: Fully Automatic Software-Hardware Co-Design Framework for Low-Bit Vision Transformer,https://arxiv.org/abs/2201.06618,https://arxiv.org/abs/2201.06618,https://arxiv.org/pdf/2201.06618.pdf,False,True,,Computer Science > Machine Learning,DEADBEEF
2022,ACM,article,__no_data__,['BERT'],__no_data__,"Varuna: Scalable, Low-cost Training of Massive Deep Learning Models",https://doi.org/10.1145/3492321.3519584,https://doi.org/10.1145/3492321.3519584,False,False,check,,Proceedings of the Seventeenth European Conference on Computer Systems,DEADBEEF
2022,IEEE,article,__no_data__,['BERT'],__no_data__,ViA: A Novel Vision-Transformer Accelerator Based on FPGA,https://doi.org/10.1109/TCAD.2022.3197489,https://doi.org/10.1109/TCAD.2022.3197489,None,False,True,,IEEE Transactions on Computer-Aided Design of Integrated Circuits and System[s,DEADBEEF
2021,Arxiv,article,__no_data__,['BERT'],__no_data__,Vis-TOP: Visual Transformer Overlay Processor,https://arxiv.org/abs/2110.10957,https://arxiv.org/abs/2110.10957,https://arxiv.org/pdf/2110.10957.pdf,False,True,,Computer Science > Computer Vision and Pattern Recognition,DEADBEEF
2023,IEEE,article,__no_data__,['BERT'],__no_data__,ViTALiTy: Unifying Low-rank and Sparse Approximation for Vision Transformer Acceleration with a Linear Taylor Attention,https://doi.org/10.1109/HPCA56546.2023.10071081,https://doi.org/10.1109/HPCA56546.2023.10071081,https://arxiv.org/pdf/2211.05109.pdf,False,True,,International Symposium on High-Performance Computer Architecture (HPCA),DEADBEEF
2022,IEEE,article,__no_data__,['BERT'],__no_data__,Work-in-Progress: Utilizing latency and accuracy predictors for efficient hardware-aware NAS,https://doi.org/10.1109/CODES-ISSS55005.2022.00014,https://doi.org/10.1109/CODES-ISSS55005.2022.00014,False,False,True,,International Conference on Hardware/Software Codesign and System Synthesis (CODES+ISSS),DEADBEEF
2022,None,article,__no_data__,['BERT'],__no_data__,XTC: Extreme Compression for Pre-trained Transformers Made Simple and Efficient,None,None,https://proceedings.neurips.cc/paper_files/paper/2022/file/1579d5d8edacd85ac1a86aea28bdf32d-Paper-Conference.pdf,False,True,,None,DEADBEEF
2022,None,article,__no_data__,['BERT'],__no_data__,ZeroQuant: Efficient and Affordable Post-Training Quantization for Large-Scale Transformers,None,None,False,False,True,,None,DEADBEEF
2022,None,article,__no_data__,['BERT'],__no_data__,Fully Unsupervised Machine Translation Using Context-Aware Word Translation and Denoising Autoencoder,https://doi.org/10.1080/08839514.2022.2031817,https://doi.org/10.1080/08839514.2022.2031817,https://www.tandfonline.com/doi/pdf/10.1080/08839514.2022.2031817,False,False,,None,DEADBEEF
2022,IEEE,article,__no_data__,['BERT'],__no_data__,DistilHuBERT: Speech representation learning by layer-wise distillation of hidden-unit BERT,https://doi.org/10.1109/ICASSP43922.2022.9747490,https://doi.org/10.1109/ICASSP43922.2022.9747490,https://arxiv.org/pdf/2110.01900.pdf,True,False,,"IEEE International Conference on Acoustics, Speech and Signal Processing",DEADBEEF
2022,IEEE,article,__no_data__,['BERT'],__no_data__,Data Movement Reduction for DNN Accelerators: Enabling Dynamic Quantization Through an eFPGA,https://doi.org/10.1109/ISVLSI54635.2022.00082,https://doi.org/10.1109/ISVLSI54635.2022.00082,https://scholar.archive.org/work/hd53tmr62rhn3mxplkjzhrnvw4/access/wayback/https://publikationen.bibliothek.kit.edu/1000151937/149523013,True,True,,Computer Society Annual Symposium on VLSI,DEADBEEF
2021,IEEE,article,__no_data__,['BERT'],__no_data__,Elbert: Fast albert with confidence-window based early exit,https://doi.org/10.1109/ICASSP39728.2021.9414572,https://doi.org/10.1109/ICASSP39728.2021.9414572,https://arxiv.org/pdf/2107.00175.pdf,False,False,,"IEEE International Conference on Acoustics, Speech and Signal Processing",DEADBEEF
2021,ACL,article,__no_data__,['BERT'],__no_data__,Ghostbert: Generate more features with cheap operations for BERT,http://dx.doi.org/10.18653/v1/2021.acl--long.509,http://dx.doi.org/10.18653/v1/2021.acl-long.509,https://aclanthology.org/2021.acl-long.509.pdf,False,True,,International Joint Conference on Natural Language Processing,DEADBEEF
2022,AIMPress,article,__no_data__,['BERT'],__no_data__,Hardware-friendly compression and hardware acceleration for transformer: A survey,https://www.aimspress.com/article/doi/10.3934/era.2022192,https://www.aimspress.com/article/doi/10.3934/era.2022192,https://www.aimspress.com/aimspress-data/era/2022/10/PDF/era-30-10-192.pdf,False,True,,Electronic Research Archive,DEADBEEF
2022,HiPEAC,article,__no_data__,['BERT'],__no_data__,Hardware/Software Co-Design of Edge DNN Accelerators with TFLite,https://doi.org/10.1007/978--3--030--87208--8_5,https://https://eprints.gla.ac.uk/280378/,False,False,True,,International Summer School on Advanced Computer Architecture and Compilation for High-Performance and Embedded Systems,DEADBEEF
2020,Arixv,article,__no_data__,['BERT'],__no_data__,Towards Fully 8-bit Integer Inference for the Transformer Model,https://arxiv.org/abs/2009.08034,https://arxiv.org/abs/2009.08034,https://arxiv.org/pdf/2009.08034.pdf,False,True,,Computer Science > Computation and Language,DEADBEEF
2023,Arxiv,article,__no_data__,['BERT'],__no_data__,ViTA: A Vision Transformer Inference Accelerator for Edge Applications,https://doi.org/10.48550/arXiv.2302.09108,https://doi.org/10.48550/arXiv.2302.09108,https://arxiv.org/pdf/2302.09108.pdf,False,True,,Computer Science > Hardware Architecture,DEADBEEF
2023,Elsevier,article,__no_data__,['BERT'],__no_data__,Trends in AI inference energy consumption: Beyond the performance-vs-parameter laws of deep learning,https://doi.org/10.1016/j.suscom.2023.100857,https://doi.org/10.1016/j.suscom.2023.100857,https://www.sciencedirect.com/science/article/pii/S2210537923000124/pdfft?md5=4bec2735c1586b935287e6afea9e63a2&pid=1-s2.0-S2210537923000124-main.pdf,False,True,,Sustainable Computing: Informatics and Systems,DEADBEEF
2023,Arxiv,article,__no_data__,['BERT'],__no_data__,TRON: Transformer Neural Network Acceleration with Non-Coherent Silicon Photonics,https://doi.org/10.48550/arXiv.2303.12914,https://doi.org/10.48550/arXiv.2303.12914,https://arxiv.org/pdf/2303.12914.pdf,False,True,,Computer Science > Machine Learning,DEADBEEF
2023,Arxiv,article,__no_data__,['BERT'],__no_data__,TransCODE: Co-design of Transformers and Accelerators for Efficient Training and Inference,https://doi.org/10.48550/arXiv.2303.14882,https://doi.org/10.48550/arXiv.2303.14882,https://arxiv.org/pdf/2303.14882,False,True,,Computer Science > Machine Learning,DEADBEEF
2021,IEEE,article,__no_data__,['BERT'],__no_data__,ProtTrans: Toward Understanding the Language of Life Through Self-Supervised Learning,https://doi.org/10.1109/TPAMI.2021.3095381,https://doi.org/10.1109/TPAMI.2021.3095381,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9477085,False,True,,IEEE Transactions on Pattern Analysis and Machine Intelligence,DEADBEEF
2021,Arxiv,article,__no_data__,['BERT'],__no_data__,Prune once for all: Sparse pre-trained language models,https://arxiv.org/abs/2111.05754,https://arxiv.org/abs/2111.05754,https://arxiv.org/pdf/2111.05754.pdf,False,True,,Computer Science > Computation and Language,DEADBEEF
2021,None,article,__no_data__,['BERT'],__no_data__,ROSITA: Refined BERT cOmpreSsion with InTegrAted techniques,https://doi.org/10.1609/aaai.v35i10.17056,https://doi.org/10.1609/aaai.v35i10.17056,https://ojs.aaai.org/index.php/AAAI/article/download/17056/16863,False,check,,None,DEADBEEF
2023,IEEE,article,__no_data__,['BERT'],__no_data__,TinyVers: A Tiny Versatile System-on-chip with State-Retentive eMRAM for ML Inference at the Extreme Edge,https://doi.org/10.1109/JSSC.2023.3236566,https://doi.org/10.1109/JSSC.2023.3236566,https://arxiv.org/pdf/2301.03537.pdf,ignore,True,,IEEE Journal of Solid-State Circuits,DEADBEEF
2020,Arxiv,article,__no_data__,['BERT'],__no_data__,TopicBERT for energy efficient document classification,https://doi.org/10.48550/arXiv.2010.16407,https://doi.org/10.48550/arXiv.2010.16407,https://arxiv.org/pdf/2010.16407.pdf,False,wip,,Computer Science > Computation and Language,DEADBEEF
2021,MLSys,article,__no_data__,['BERT'],__no_data__,VS-Quant: Per-vector Scaled Quantization for Accurate Low-Precision Neural Network Inference,https://proceedings.mlsys.org/paper_files/paper/2021,https://proceedings.mlsys.org/paper_files/paper/2021,https://proceedings.mlsys.org/paper_files/paper/2021/file/48a6431f04545e11919887748ec5cb52-Paper.pdf,False,True,,Proceedings of Machine Learning and Systems,DEADBEEF
2022,ACM,article,__no_data__,['BERT'],__no_data__,Workload-Balanced Graph Attention Network Accelerator with Top-K Aggregation Candidates,https://doi.org/10.1145/3508352.3549343,https://doi.org/10.1145/3508352.3549343,False,False,check,,International Conference on Computer-Aided Design,DEADBEEF
2023,HarwardLibrary,article,__no_data__,['BERT'],__no_data__,Architecting High Performance Silicon Systems for Accurate and Efficient On-Chip Deep Learning,https://nrs.harvard.edu/URN--3:HUL.INSTREPOS:37375806,https://nrs.harvard.edu/URN-3:HUL.INSTREPOS:37375806,https://dash.harvard.edu/bitstream/handle/1/37375806/Final_Draft_PhD_Dissertation_Thierry_Tambe.pdf,False,True,,Harvard University Graduate School of Arts and Sciences,DEADBEEF
2023,IEEE,article,__no_data__,['BERT'],__no_data__,Hardware-efficient Softmax Approximation for Self-Attention Networks,https://doi.org/10.1109/ISCAS46773.2023.10181465,https://doi.org/10.1109/ISCAS46773.2023.10181465,False,False,True,,IEEE International Symposium on Circuits and Systems (ISCAS),DEADBEEF
2023,Arxiv,article,__no_data__,['BERT'],__no_data__,Fast Prototyping Next-Generation Accelerators for New ML Models using MASE: ML Accelerator System Exploration,https://arxiv.org/abs/2307.15517,https://arxiv.org/abs/2307.15517,https://arxiv.org/pdf/2307.15517.pdf,False,True,,Computer Science > Hardware Architecture,DEADBEEF
2023,Wiley,article,__no_data__,['BERT'],__no_data__,Advances in Electromagnetics Empowered by Artificial Intelligence and Deep Learning,ISBN:9781119853893,https://books.google.com/books?id=rlPNEAAAQBAJ,False,False,True,,IEEE Press Series on Electromagnetic Wave Theory,DEADBEEF
2023,IEEE,article,__no_data__,['BERT'],__no_data__,A Scalable GPT-2 Inference Hardware Architecture on FPGA,https://doi.org/10.1109/IJCNN54540.2023.10191067,https://doi.org/10.1109/IJCNN54540.2023.10191067,False,False,True,,International Joint Conference on Neural Networks (IJCNN),DEADBEEF
2023,IEEE,article,__no_data__,['BERT'],__no_data__,BL-PIM: Varying the Burst Length to Realize the All-Bank Performance and Minimize the Multi-Workload Interference for in-DRAM PIM,https://doi.org/10.1109/ACCESS.2023.3300893,https://doi.org/10.1109/ACCESS.2023.3300893,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10198428,False,True,,IEEE Access,DEADBEEF
2023,SydneyDigital,article,__no_data__,['BERT'],__no_data__,Integrated Transformers Inference Framework for Multiple Tenants on GPU,https://hdl.handle.net/2123/31606,https://hdl.handle.net/2123/31606,https://ses.library.usyd.edu.au/bitstream/handle/2123/31606/Thesis__Yuning_Zhang%20%281%29.pdf?sequence=2&isAllowed=y,False,True,,Sydney Digital Theses,DEADBEEF
2023,IEEE,article,__no_data__,['BERT'],__no_data__,Embedded Deep Learning Accelerators: A Survey on Recent Advances,https://doi.org/10.1109/TAI.2023.3311776,https://doi.org/10.1109/TAI.2023.3311776,False,False,True,,IEEE Transactions on Artificial Intelligence ,DEADBEEF
2023,TSpace,article,__no_data__,['BERT'],__no_data__,Collective Communication Enabled Transformer Acceleration on Heterogeneous Clusters,https://hdl.handle.net/1807/130585,https://hdl.handle.net/1807/130585,https://tspace.library.utoronto.ca/bitstream/1807/130585/3/Gao_Yu_202311_MAS_thesis.pdf,False,True,T,IEEE Transactions on Artificial Intelligence ,DEADBEEF
2023,IEEE,article,FPGA,['BERT'],__no_data__,FET-OPU: A Flexible and Efficient FPGA-Based Overlay Processor for Transformer Networks,https://doi.org/10.1109/ICCAD57390.2023.10323752,https://doi.org/10.1109/ICCAD57390.2023.10323752,False,False,True,ICCAD,IEEE/ACM International Conference on Computer Aided Design (ICCAD),DEADBEEF
2023,ACADLore,article,FPGA,['BERT'],__no_data__,Racism and Hate Speech Detection on Twitter: A QAHA-Based Hybrid Deep Learning Approach Using LSTM-CNN,https://doi.org/10.56578/ijkis010202,https://doi.org/10.56578/ijkis010202,https://library.acadlore.com/IJKIS/2023/1/2/IJKIS_01.02_02.pdf,False,True,,International Journal of Knowledge and Innovation Studies,DEADBEEF
2023,IEEE,article,asic,"['LLm', 'Transformer']",__no_data__,"22.9 A 12nm 18.1TFLOPs/W Sparse Transformer Processor with Entropy-Based Early Exit, Mixed-Precision Predication and Fine-Grained Power Management",https://doi.org/10.1109/ISSCC42615.2023.10067817,https://doi.org/10.1109/ISSCC42615.2023.10067817,False,True,True,ISSCC,IEEE International Solid-State Circuits Conference,DEADBEEF
2023,IEEE,article,FPGA,"['LLm', 'Transformer']",__no_data__,P^3 ViT: A CIM-Based High-Utilization Architecture With Dynamic Pruning and Two-Way Ping-Pong Macro for Vision Transformer,https://doi.org/10.1109/TCSI.2023.3315060,https://doi.org/10.1109/TCSI.2023.3315060,False,True,True,TCSI,IEEE Transactions on Circuits and Systems,DEADBEEF
2023,IEEE/CVF,article,algoritm,"['LLm', 'Transformer']",__no_data__,I-ViT: Integer-only Quantization for Efficient Vision Transformer Inference,https://arxiv.org/abs/2207.01405,https://arxiv.org/abs/2207.01405,https://arxiv.org/pdf/2207.01405,True,True,TCSI,IEEE/CVF International Conference on Computer Vision,DEADBEEF
2023,NanyangTechnologicalUniversity,thesis,FPGA,['Transformer'],"['in-memory-processing', 'pruning']",Enabling efficient edge intelligence: a hardware-software codesign approach,https://dr.ntu.edu.sg/handle/10356/172499,https://dr.ntu.edu.sg/handle/10356/172499,https://dr.ntu.edu.sg/bitstream/10356/172499/2/Thesis_Final_HUAISHUO.pdf,False,True,,Nanyang Technological University,DEADBEEF
2023,IEEE,article,FPGA,[''],__no_data__,Automatic Kernel Generation for Large Language Models on Deep Learning Accelerators,https://doi.org/10.1109/ICCAD57390.2023.10323944,https://doi.org/10.1109/ICCAD57390.2023.10323944,False,False,True,ICCAD,IEEE/ACM International Conference on Computer Aided Design (ICCAD),DEADBEEF
2023,IEEE,article,FPGA,[''],__no_data__,A Low-Latency and Lightweight FPGA-Based Engine for Softmax and Layer Normalization Acceleration,https://doi.org/10.1109/ICCE--Asia59966.2023.10326397,https://doi.org/10.1109/ICCE-Asia59966.2023.10326397,False,False,True,,IEEE International Conference on Consumer Electronics-Asia (ICCE-Asia),DEADBEEF
2023,IEEE,article,FPGA,['Transformer'],__no_data__,PP-Transformer: Enable Efficient Deployment of Transformers Through Pattern Pruning,https://doi.org/10.1109/ICCAD57390.2023.10323836,https://doi.org/10.1109/ICCAD57390.2023.10323836,False,False,True,ICCAD,IEEE/ACM International Conference on Computer Aided Design (ICCAD),DEADBEEF
2023,Arxive,arx,FPGA,['Transformer'],__no_data__,DEAP: Design Space Exploration for DNN Accelerator Parallelism,https://arxiv.org/abs/2312.15388,https://arxiv.org/abs/2312.15388,https://arxiv.org/pdf/2312.15388.pdf,False,True,,"Computer Science > Distributed, Parallel, and Cluster Computing",DEADBEEF
2023,Arxive,arx,FPGA,"['Transformer', 'LLM']",__no_data__,Understanding the Potential of FPGA-Based Spatial Acceleration for Large Language Model Inference,https://arxiv.org/abs/2312.15159,https://arxiv.org/abs/2312.15159,https://arxiv.org/pdf/2312.15159.pdf,False,True,,Computer Science > Machine Learning,DEADBEEF
2023,IEEE,article,FPGA,['Transformer'],__no_data__,An RRAM-Based Computing-in-Memory Architecture and Its Application in Accelerating Transformer Inference,https://doi.ieeecomputersociety.org/10.1109/TVLSI.2023.3345651,https://doi.ieeecomputersociety.org/10.1109/TVLSI.2023.3345651,False,False,True,TVLSI,IEEE Transactions on Very Large Scale Integration (VLSI) Systems,DEADBEEF
2023,IEEE,article,FPGA,['Transformer'],__no_data__,Mobile Transformer Accelerator Exploiting Various Line Sparsity and Tile-Based Dynamic Quantization,https://doi.org/10.1109/TCAD.2023.3347291,https://doi.org/10.1109/TCAD.2023.3347291,False,False,True,TCAD,IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems,DEADBEEF
2023,IEEE,article,FPGA,['Transformer'],__no_data__,A Lightweight Transformer Model using Neural ODE for FPGAs,https://doi.org/10.1109/IPDPSW59300.2023.00029,https://doi.org/10.1109/IPDPSW59300.2023.00029,False,False,True,IPDPSW,IEEE International Parallel and Distributed Processing Symposium Workshops,DEADBEEF
2024,Arxive,article,FPGA,['Transformer'],__no_data__,A Cost-Efficient FPGA Implementation of Tiny Transformer Model using Neural ODE,https://arxiv.org/abs/2401.02721,https://arxiv.org/abs/2401.02721,https://arxiv.org/pdf/2401.02721.pdf,False,True,,Computer Science > Machine Learning,DEADBEEF
2024,Arxive,article,FPGA,['Transformer'],__no_data__,FlightLLM: Efficient Large Language Model Inference with a Complete Mapping Flow on FPGAs,https://arxiv.org/abs/2401.03868,https://arxiv.org/abs/2401.03868,https://arxiv.org/pdf/2401.03868.pdf,False,True,,Computer Science > Hardware Architecture,DEADBEEF
2024,Arxive,article,FPGA,"['Transformer', 'LLM']",__no_data__,Accelerating Neural Networks for Large Language Models and Graph Processing with Silicon Photonics,https://arxiv.org/abs/2401.06885,https://arxiv.org/abs/2401.06885,https://arxiv.org/pdf/2401.06885.pdf,False,True,,Computer Science > Hardware Architecture,DEADBEEF
2024,IEEE,article,FPGA,"['Transformer', 'LLM']",__no_data__,Quantization and Hardware Architecture Co-Design for Matrix-Vector Multiplications of Large Language Models,https://doi.org/10.1109/TCSI.2024.3350661,https://doi.org/10.1109/TCSI.2024.3350661,False,False,True,TCSI,IEEE Transactions on Circuits and Systems I: Regular Papers,DEADBEEF
2024,IEEE,article,FPGA,"['Transformer', 'RISC-V', 'PIM', 'Memory']",__no_data__,RDCIM: RISC-V Supported Full-Digital Computing-in-Memory Processor With High Energy Efficiency and Low Area Overhead,https://doi.org/10.1109/TCSI.2024.3350664,https://doi.org/10.1109/TCSI.2024.3350664,False,False,True,TCSI,IEEE Transactions on Circuits and Systems I: Regular Papers,DEADBEEF
2024,Arxive,article,FPGA,['Transformer'],__no_data__,A Survey on Hardware Accelerators for Large Language Models,https://arxiv.org/abs/2401.09890,https://arxiv.org/abs/2401.09890,https://arxiv.org/pdf/2401.09890.pdf,False,True,,Computer Science > Hardware Architecture,DEADBEEF
2024,Arxive,article,FPGA,['Transformer'],__no_data__,BETA: Binarized Energy-Efficient Transformer Accelerator at the Edge,https://arxiv.org/abs/2401.11851,https://arxiv.org/abs/2401.11851,https://arxiv.org/abs/2401.11851.pdf,False,True,,Computer Science > Hardware Architecture,DEADBEEF
2024,Arxive,article,FPGA,['Transformer'],__no_data__,AttentionLego: An Open-Source Building Block For Spatially-Scalable Large Language Model Accelerator With Processing-In-Memory Technology,https://arxiv.org/abs/2401.11459,https://arxiv.org/abs/2401.11459,https://arxiv.org/abs/2401.11459.pdf,False,True,,Computer Science > Hardware Architecture,DEADBEEF
2024,Arxive,article,FPGA,['Transformer'],__no_data__,SSR: Spatial Sequential Hybrid Architecture for Latency Throughput Tradeoff in Transformer Acceleration,https://arxiv.org/abs/2401.10417,https://arxiv.org/abs/2401.10417,https://arxiv.org/abs/2401.10417.pdf,False,True,,Computer Science > Hardware Architecture,DEADBEEF
2024,Arxiv,article,PIM,['Transformer'],__no_data__,CIM-MLC: A Multi-level Compilation Stack for Computing-In-Memory Accelerators,https://arxiv.org/abs/2401.12428,https://arxiv.org/abs/2401.12428,https://arxiv.org/pdf/2401.12428.pdf,False,True,,Computer Science > Hardware Architecture,DEADBEEF
2024,Arxiv,article,PIM,['Transformer'],__no_data__,CIM-MLC: A Multi-level Compilation Stack for Computing-In-Memory Accelerators,https://arxiv.org/abs/2401.12428,https://arxiv.org/abs/2401.12428,https://arxiv.org/pdf/2401.12428.pdf,False,True,,Computer Science > Hardware Architecture,DEADBEEF
2024,IEEE,review,PIM,['Transformer'],__no_data__,The Era of Generative Artificial Intelligence: In-Memory Computing Perspective,https://doi.org/10.1109/IEDM45741.2023.10413786,https://doi.org/10.1109/IEDM45741.2023.10413786,False,False,True,IEDM,International Electron Devices Meeting (IEDM),DEADBEEF
2024,Arxiv,article,FPGA,['Transformer'],__no_data__,Hydragen: High-Throughput LLM Inference with Shared Prefixes,https://arxiv.org/abs/2402.05099,https://arxiv.org/abs/2402.05099,https://arxiv.org/pdf/2402.05099.pdf,False,True,,Computer Science > Machine Learning,DEADBEEF
2024,Arxiv,survey,FPGA,['Transformer'],__no_data__,A Survey on Transformer Compression,https://arxiv.org/abs/2402.05964.pdf,https://arxiv.org/abs/2402.05964.pdf,https://arxiv.org/pdf/2402.05964.pdf,False,True,,Computer Science > Machine Learning,DEADBEEF
2024,Arxiv,article,FPGA,['Transformer'],__no_data__,SLEB: Streamlining LLMs through Redundancy Verification and Elimination of Transformer Blocks,https://arxiv.org/abs/2402.09025,https://arxiv.org/abs/2402.09025,https://arxiv.org/pdf/2402.09025.pdf,False,True,,,DEADBEEF
2024,Arxiv,article,FPGA,['Transformer'],__no_data__,Stochastic Spiking Attention: Accelerating Attention with Stochastic Computing in Spiking Networks,https://arxiv.org/abs/2402.09109,https://arxiv.org/abs/2402.09109,https://arxiv.org/pdf/2402.09109.pdf,False,True,,Computer Science > Hardware Architecture,DEADBEEF
2024,Arxiv,article,FPGA,['Transformer'],__no_data__,Reusing Softmax Hardware Unit for GELU Computation in Transformers,https://arxiv.org/abs/2402.10118,https://arxiv.org/abs/2402.10118,https://arxiv.org/abs/2402.10118.pdf,False,True,,Computer Science > Hardware Architecture,DEADBEEF
2024,Arxiv,article,FPGA,['Transformer'],__no_data__,ConSmax: Hardware-Friendly Alternative Softmax with Learnable Parameters,https://arxiv.org/abs/2402.10930,https://arxiv.org/abs/2402.10930,https://arxiv.org/pdf/2402.10930.pdf,False,True,,Computer Science > Hardware Architecture,DEADBEEF
2024,Arxiv,article,FPGA,['Transformer'],__no_data__,Speculative Streaming: Fast LLM Inference without Auxiliary Models,https://arxiv.org/abs/2402.11131,https://arxiv.org/abs/2402.11131,https://arxiv.org/pdf/2402.11131.pdf,False,True,,Computer Science > Computation and Language,DEADBEEF
2024,ACM,article,3D,['Transformer'],__no_data__,H3D-Transformer: A Heterogeneous 3D (H3D) Computing Platform for Transformer Model Acceleration on Edge Devices,https://dl.acm.org/doi/10.1145/3649219,https://dl.acm.org/doi/10.1145/3649219,https://dl.acm.org/doi/pdf/10.1145/3649219,False,True,,ACM Transactions on Design Automation of Electronic Systems,DEADBEEF
2024,Arxiv,article,FPGA,['Transformer'],__no_data__,NeuPIMs: NPU-PIM Heterogeneous Acceleration for Batched LLM Inferencing,https://arxiv.org/abs/2403.00579,https://arxiv.org/abs/2403.00579,https://arxiv.org/pdf/2403.00579.pdf,False,True,,Computer Science > Hardware Architecture,DEADBEEF
2024,ACM,article,FPGA,['Transformer'],__no_data__,Cerberus: Triple Mode Acceleration of Sparse Matrix and Vector Multiplication,https://doi.org/10.1145/3653020,https://doi.org/10.1145/365302000579,https://dl.acm.org/doi/pdf/10.1145/3653020,False,True,,Transactions on Architecture and Code Optimization,DEADBEEF
2024,Arxiv,article,GPU,['Transformer'],__no_data__,DEFA: Efficient Deformable Attention Acceleration via Pruning-Assisted Grid-Sampling and Multi-Scale Parallel Processing,https://arxiv.org/abs/2403.10913,https://arxiv.org/abs/2403.10913,https://arxiv.org/pdf/2403.10913.pdf,False,True,,Computer Science > Hardware Architecture,DEADBEEF
2024,Arxiv,article,GPU,['Transformer'],__no_data__,FastDecode: High-Throughput GPU-Efficient LLM Serving using Heterogeneous Pipelines,https://arxiv.org/abs/2403.11421,https://arxiv.org/abs/2403.11421,https://arxiv.org/pdf/2403.11421.pdf,False,True,,"Distributed, Parallel, and Cluster Computing",DEADBEEF
2024,Arxiv,article,FPGA,['Transformer'],__no_data__,Accelerating ViT Inference on FPGA through Static and Dynamic Pruning,https://arxiv.org/abs/2403.14047,https://arxiv.org/abs/2403.14047,https://arxiv.org/pdf/2403.14047.pdf,False,True,,"Distributed, Parallel, and Cluster Computing",DEADBEEF
2024,Arxiv,article,PIM,['Transformer'],__no_data__,Allspark: Workload Orchestration for Visual Transformers on Processing In-Memory Systems,https://arxiv.org/abs/2403.15069,https://arxiv.org/abs/2403.15069,https://arxiv.org/pdf/2403.15069.pdf,False,True,,Computer Science > Hardware Architecture,DEADBEEF
2024,IEEE,article,FPGA,['Transformer'],__no_data__,Impact of High-Level-Synthesis on Reliability of Artificial Neural Network Hardware Accelerators,https://doi.org/10.1109/TNS.2024.3377596,https://doi.org/10.1109/TNS.2024.3377596,https://inria.hal.science/hal-04514579/file/TNS2024_HLS.pdf,False,True,,IEEE Transactions on Nuclear Science,DEADBEEF
2024,Arxiv,article,FPGA,['Transformer'],__no_data__,An FPGA-Based Reconfigurable Accelerator for Convolution-Transformer Hybrid EfficientViT,https://arxiv.org/abs/2403.20230,https://arxiv.org/abs/2403.20230,https://arxiv.org/pdf/2403.20230.pdf,False,True,,Computer Science > Hardware Architecture,DEADBEEF
2024,IEEE,article,FPGA,['Transformer'],__no_data__,TransFRU: Efficient Deployment of Transformers on FPGA with Full Resource Utilization,https://doi.org/10.1109/ASP--DAC58780.2024.10473976,https://doi.org/10.1109/ASP-DAC58780.2024.10473976,False,False,True,ASP-DAC,Asia and South Pacific Design Automation Conference (ASP-DAC),DEADBEEF
2024,IEEE,article,FPGA,['Transformer'],__no_data__,PRIMATE: Processing in Memory Acceleration for Dynamic Token-pruning Transformers,https://doi.org/10.1109/ASP--DAC58780.2024.10473968,https://doi.org/10.1109/ASP-DAC58780.2024.10473968,False,False,True,ASP-DAC,Asia and South Pacific Design Automation Conference (ASP-DAC),DEADBEEF
2024,IEEE,article,FPGA,['Transformer'],__no_data__,SWAT: An Efficient Swin Transformer Accelerator Based on FPGA,https://doi.org/10.1109/ASP--DAC58780.2024.10473931,https://doi.org/10.1109/ASP-DAC58780.2024.10473931,False,False,True,ASP-DAC,Asia and South Pacific Design Automation Conference (ASP-DAC),DEADBEEF
2024,Arxiv,article,FPGA,['Transformer'],__no_data__,VTR: An Optimized Vision Transformer for SAR ATR Acceleration on FPGA,https://arxiv.org/abs/2404.04527v1,https://arxiv.org/abs/2404.04527v1,https://arxiv.org/pdf/2404.04527v1,False,True,,Computer Science > Computer Vision and Pattern Recognition,DEADBEEF
2024,Arxiv,article,FPGA,['Transformer'],__no_data__,Workload-Aware Hardware Accelerator Mining for Distributed Deep Learning Training,https://arxiv.org/abs/2404.14632v1,https://arxiv.org/abs/2404.14632v1,https://arxiv.org/pdf/2404.14632v1,False,True,,Computer Science > Hardware Architecture,DEADBEEF
2024,Arxiv,article,framework|algorithm,"['LLm', 'EDA']",__no_data__,QServe: W4A8KV4 Quantization and System Co-design for Efficient LLM Serving,https://arxiv.org/abs/2405.04532,https://arxiv.org/abs/2405.04532,https://arxiv.org/pdf/2405.04532,True,True,,Computer Science > Computation and Language,DEADBEEF
2024,ACM,article,PIM,['Transformer'],__no_data__,NeuPIMs: NPU-PIM Heterogeneous Acceleration for Batched LLM Inferencing,https://dl.acm.org/doi/abs/10.1145/3620666.3651380,https://dl.acm.org/doi/abs/10.1145/3620666.3651380,https://dl.acm.org/doi/pdf/10.1145/3620666.3651380,False,True,,International Conference on Architectural Support for Programming Languages and Operating Systems,DEADBEEF
2024,ACM,article,PIMGPU,['Transformer'],__no_data__,VITA: ViT Acceleration for Efficient 3D Human Mesh Recovery via Hardware-Algorithm Co-Design,False,False,https://www.crcv.ucf.edu/chenchen/2024_DAC_VITA_Final.pdf,True,True,,International Conference on Architectural Support for Programming Languages and Operating Systems,DEADBEEF
2024,Arxiv,article,FPGA,"['Transformer', 'LLM']",__no_data__,HLSTransform: Energy-Efficient Llama 2 Inference on FPGAs Via High Level Synthesis,https://arxiv.org/abs/2405.00738,https://arxiv.org/abs/2405.00738,https://arxiv.org/pdf/2405.00738,True,True,,Computer Science > Hardware Architecture,DEADBEEF
2024,Arxiv,article,ACC,['AI'],__no_data__,SCAR: Scheduling Multi-Model AI Workloads on Heterogeneous Multi-Chiplet Module Accelerators,https://arxiv.org/abs/2405.00790,https://arxiv.org/abs/2405.00790,https://arxiv.org/pdf/2405.00790,True,True,,Computer Science > Hardware Architecture,DEADBEEF
2024,Arxiv,article,ACC,"['VIT', 'Transformer']",__no_data__,Trio-ViT: Post-Training Quantization and Acceleration for Softmax-Free Efficient Vision Transformer,https://arxiv.org/abs/2405.03882,https://arxiv.org/abs/2405.03882,https://arxiv.org/pdf/2405.03882,True,True,,Computer Science > Computer Vision and Pattern Recognition,DEADBEEF
2024,Arxiv,article,Memory,"['LLm', 'Transformer']",__no_data__,SambaNova SN40L: Scaling the AI Memory Wall with Dataflow and Composition of Experts,https://arxiv.org/abs/2405.07518,https://arxiv.org/abs/2405.07518,https://arxiv.org/pdf/2405.07518,True,True,,Computer Science > Hardware Architecture,DEADBEEF
2024,IEEE,article,EDA,"['LLm', 'EDA']",__no_data__,TensorMap: A Deep RL-Based Tensor Mapping Framework for Spatial Accelerators,https://doi.ieeecomputersociety.org/10.1109/TC.2024.3398424,https://doi.ieeecomputersociety.org/10.1109/TC.2024.3398424,False,True,True,IEEE Transactions on Computers,TC,DEADBEEF
2024,MLSys,article,EDA,"['LLm', 'EDA']",__no_data__,JIT-Q: Just-in-time Quantization with Processing-In-Memory for Efficient ML Training,https://arxiv.org/abs/2311.05034,https://arxiv.org/abs/2311.05034,https://proceedings.mlsys.org/paper_files/paper/2024/file/136b9a13861308c8948cd308ccd02658-Paper-Conference.pdf,True,True,Proceedings of Machine Learning and Systems 6 (MLSys 2024),Proceedings of Machine Learning and Systems 6 (MLSys 2024),DEADBEEF
2024,IEEE,article,algorithm,"['LLm', 'ViT']",__no_data__,DCT-ViT: High-Frequency Pruned Vision Transformer with Discrete Cosine Transform,https://doi.org/10.1109/ACCESS.2024.3410231,https://doi.org/10.1109/ACCESS.2024.3410231,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10549904,True,True,Access,IEEE Access,DEADBEEF
2023,ACM,article,TPU,['LLM'],__no_data__,TPU v4: An Optically Reconfigurable Supercomputer for Machine Learning with Hardware Support for Embeddings,https://doi.org/10.1145/3579371.3589350,https://doi.org/10.1145/3579371.3589350,https://dl.acm.org/doi/pdf/10.1145/3579371.3589350,True,True,ISCA,ACM International Symposium on Computer Architecture,DEADBEEF
2024,Arxiv,article,framework,"['LLm', 'Transformer']",__no_data__,TransAxx: Efficient Transformers with Approximate Computing,https://arxiv.org/abs/2402.07545,https://arxiv.org/abs/2402.07545,https://arxiv.org/pdf/2402.07545,True,True,,Computer Science > Machine Learning,DEADBEEF
2023,PMLR,article,platform,"['LLm', 'Transformer']",__no_data__,FlexGen: High-Throughput Generative Inference of Large Language Models with a Single GPU,https://proceedings.mlr.press/v202/sheng23a,https://proceedings.mlr.press/v202/sheng23a,https://proceedings.mlr.press/v202/sheng23a/sheng23a.pdf,True,True,PMLR,Proceedings of Machine Learning Research,DEADBEEF
2024,Arxiv,article,ASIC,"['LLm', 'Transformer']",__no_data__,ITA: An Energy-Efficient Attention and Softmax Accelerator for Quantized Transformers,https://arxiv.org/abs/2307.03493,https://arxiv.org/abs/2307.03493,https://arxiv.org/pdf/2307.03493,True,True,,Computer Science > Hardware Architecture,DEADBEEF
2023,Arxiv,article,ASIC,"['LLm', 'Transformer']",__no_data__,ITA: An Energy-Efficient Attention and Softmax Accelerator for Quantized Transformers,https://arxiv.org/abs/2307.03493,https://arxiv.org/abs/2307.03493,https://arxiv.org/pdf/2307.03493,True,True,,Computer Science > Hardware Architecture,DEADBEEF
2023,Arxiv,article,PIM,"['LLm', 'Transformer']",__no_data__,X-Former: In-Memory Acceleration of Transformers,https://arxiv.org/abs/2303.07470,https://arxiv.org/abs/2303.07470,https://arxiv.org/pdf/2303.07470,True,True,,Computer Science > Machine Learning,DEADBEEF
2023,Arxiv,article,framework,"['LLm', 'EDA']",__no_data__,GPT4AIGChip: Towards Next-Generation AI Accelerator Design Automation via Large Language Models,https://arxiv.org/abs/2309.10730,https://arxiv.org/abs/2309.10730,https://arxiv.org/pdf/2309.10730,True,True,,Computer Science > Machine Learning,DEADBEEF
2023,IEEE,article,framework,"['LLm', 'Transformer']",__no_data__,HeatViT: Hardware-Efficient Adaptive Token Pruning for Vision Transformers,https://doi.org/10.1109/HPCA56546.2023.10071047,https://doi.org/10.1109/HPCA56546.2023.10071047,https://arxiv.org/pdf/2211.08110,True,True,HPCA,International Symposium on High-Performance Computer Architecture,DEADBEEF
2023,IEEE,article,framework,"['LLm', 'Transformer']",__no_data__,ViTCoD: Vision Transformer Acceleration via Dedicated Algorithm and Accelerator Co-Design,https://arxiv.org/abs/2210.09573,https://arxiv.org/abs/2210.09573,https://arxiv.org/pdf/2210.09573,True,True,HPCA,International Symposium on High-Performance Computer Architecture,DEADBEEF
2023,IEEE,article,platform,"['LLm', 'Transformer']",__no_data__,AccelTran: A Sparsity-Aware Accelerator for Dynamic Inference with Transformers,https://doi.org/10.1109/TCAD.2023.3273992,https://doi.org/10.1109/TCAD.2023.3273992,https://arxiv.org/pdf/2302.14705,True,True,TCAD,Transaction on Computer Aided Design,DEADBEEF
2024,Arxiv,article,Design,"['VIT', 'Transformer']",__no_data__,CGRA4ML: A Framework to Implement Modern Neural Networks for Scientific Edge Computing,https://arxiv.org/abs/2408.15561,https://arxiv.org/abs/2408.15561,https://arxiv.org/pdf/2408.15561,False,True,,Computer Science > Hardware Architecture,DEADBEEF
2024,Arxiv,article,Design,"['FPGA', 'Transformer']",__no_data__,ProTEA: Programmable Transformer Encoder Acceleration on FPGA,https://arxiv.org/abs/2409.13975,https://arxiv.org/abs/2409.13975,https://arxiv.org/pdf/2409.13975,False,True,,Computer Science > Hardware Architecture,DEADBEEF
2024,Arxiv,article,Design,"['FPGA', 'Transformer']",__no_data__,CAT: Customized Transformer Accelerator Framework on Versal ACAP,https://arxiv.org/abs/2409.09689,https://arxiv.org/abs/2409.09689,https://arxiv.org/pdf/2409.09689,False,True,,Computer Science > Hardware Architecture,DEADBEEF
2024,IEEE,article,Design,"['FPGA', 'LLM']",__no_data__,Co-design of a TinyLLM using Programmable Logic and Software on an FPGA,https://doi.org/10.1109/MWSCAS60917.2024.10658754,https://doi.org/10.1109/MWSCAS60917.2024.10658754,False,False,True,, International Midwest Symposium on Circuits and Systems (MWSCAS),DEADBEEF
2024,IEEE,article,Accelerator,"['GEMM', 'Transformer']",__no_data__,BitShare: An Efficient Precision-Scalable Accelerator with Combining-Like-Terms GEMM,RESEARCH_GATE,https://www.researchgate.net/publication/381370829_BitShare_An_Efficient_Precision-Scalable_Accelerator_with_Combining-Like-Terms_GEMM,https://www.researchgate.net/profile/Junzhong-Shen/publication/381370829_BitShare_An_Efficient_Precision-Scalable_Accelerator_with_Combining-Like-Terms_GEMM/links/666a46cba54c5f0b94613261/BitShare-An-Efficient-Precision-Scalable-Accelerator-with-Combining-Like-Terms-GEMM.pdf,True,True,ASAP,"IEEE Conference Application-Specific Systems, Architectures, and Processors",DEADBEEF
2023,?,article,algoritm,"['LLm', 'Transformer']",__no_data__,Streaming Tensor Programs: A Programming Abstraction for Streaming Dataflow Accelerators,NO_DATA,False,https://cgyurgyik.github.io/files/pubs/step-yarch.pdf,True,True,?,?,DEADBEEF
2024,IEEE,article,algoritm,"['LLm', 'Transformer']",__no_data__,SDA: Low-Bit Stable Diffusion Acceleration on Edge FPGA,NO_DATA,https://github.com/Michaela1224/SDA_code,https://www.sfu.ca/~zhenman/files/C41-FPL2024-SDA.pdf,True,True,?,?,DEADBEEF
2024,IEEE,article,algoritm,"['LLm', 'Transformer']",__no_data__,Hardware Accelerator for MobileViT Vision Transformer with Reconfigurable Computation,https://doi.org/10.1109/ISCAS58744.2024.10558190,https://doi.org/10.1109/ISCAS58744.2024.10558190,False,True,True,ISCAS,IEEE International Symposium on Circuits and Systems,DEADBEEF
2024,IEEE,article,algoritm,"['LLm', 'Transformer']",__no_data__,In-Memory Transformer Self-Attention Mechanism Using Passive Memristor Crossbar,https://doi.org/10.1109/ISCAS58744.2024.10558182,https://doi.org/10.1109/ISCAS58744.2024.10558182,False,False,True,ISCAS,IEEE International Symposium on Circuits and Systems,DEADBEEF
2024,IEEE,article,algoritm,"['LLm', 'Transformer']",__no_data__,A 3.55 mJ/frame Energy-efficient Mixed-Transformer based Semantic Segmentation Accelerator for Mobile Devices,https://doi.org/10.1109/ISCAS58744.2024.10558649,https://doi.org/10.1109/ISCAS58744.2024.10558649,False,False,True,ISCAS,IEEE International Symposium on Circuits and Systems,DEADBEEF
2024,IEEE,article,platform,"['LLm', 'Transformer']",__no_data__,FLAG: Formula-LLM-Based Auto-Generator for Baseband Hardware,https://doi.org/10.1109/ISCAS58744.2024.10558482,https://doi.org/10.1109/ISCAS58744.2024.10558482,False,False,True,ISCAS,IEEE International Symposium on Circuits and Systems,DEADBEEF
2024,IEEE,article,platform,"['LLm', 'Transformer']",__no_data__,CV-CIM: A Hybrid Domain Xor-Derived Similarity-Aware Computation-in-Memory Supporting Cost–Volume Construction,https://doi.org/10.1109/JSSC.2024.3421589,https://doi.org/10.1109/JSSC.2024.3421589,False,False,True,ISCAS,IEEE Journal of Solid-State Circuits,DEADBEEF
2024,USENIX,article,platform,"['LLm', 'Transformer']",__no_data__,LPU: A Latency-Optimized and Highly Scalable Processor for Large Language Model Inference,https://www.usenix.org/conference/osdi24/presentation/zhuang,https://www.usenix.org/conference/osdi24/presentation/zhuang,https://www.usenix.org/system/files/osdi24-zhuang.pdf,False,True,UOSDI,USENIX Symposium on Operating Systems Design and Implementation,DEADBEEF
2024,Arxiv,article,AMS,"['Analog', 'Transformer']",__no_data__,ARTEMIS: A Mixed Analog-Stochastic In-DRAM Accelerator for Transformer Neural Networks,https://arxiv.org/abs/2407.12638,https://arxiv.org/abs/2407.12638,https://arxiv.org/pdf/2407.12638,False,True,,Computer Science > Hardware Architecture,DEADBEEF
2024,Arxiv,article,platform,"['Codesign', 'Transformer']",__no_data__,CHOSEN: Compilation to Hardware Optimization Stack for Efficient Vision Transformer Inference,https://arxiv.org/abs/2407.12736,https://arxiv.org/abs/2407.12736,https://arxiv.org/pdf/2407.12736,False,True,,Computer Science > Computer Vision and Pattern Recognition,DEADBEEF
2024,Arxiv,article,platform,"['', 'Transformer']",__no_data__,Co-Designing Binarized Transformer and Hardware Accelerator for Efficient End-to-End Edge Deployment,https://arxiv.org/abs/2407.12070,https://arxiv.org/abs/2407.12070,https://arxiv.org/pdf/2407.12070,False,True,,Computer Science > Machine Learning,DEADBEEF
2024,IEEE,article,Design,"['DSA', 'LLm', 'Transformer']",__no_data__,SPSA: Exploring Sparse-Packing Computation on Systolic Arrays From Scratch,https://doi.org/10.1109/TCAD.2024.3434359,https://doi.org/10.1109/TCAD.2024.3434359,False,False,True,TCAD,IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems (,DEADBEEF
2024,IEEE,article,Design,"['DSA', 'LLm', 'Transformer']",__no_data__,SPSA: Exploring Sparse-Packing Computation on Systolic Arrays From Scratch,https://doi.org/10.1109/TCAD.2024.3434447,https://doi.org/10.1109/TCAD.2024.3434447,False,False,True,TCAD,IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems (,DEADBEEF
2024,IEEE,article,Design,"['DSA', 'LLm', 'Transformer']",__no_data__,MECLA: Memory-Compute-Efficient LLM Accelerator with Scaling Sub-matrix Partition,https://doi.org/10.1109/ISCA59077.2024.00079,https://doi.org/10.1109/ISCA59077.2024.00079,False,False,True,ISCA,ACM/IEEE 51st Annual International Symposium on Computer Architecture,DEADBEEF
2024,IEEE,article,Design,"['DSA', 'LLm', 'Transformer']",__no_data__,TCP: A Tensor Contraction Processor for AI Workloads Industrial Product,https://doi.org/10.1109/ISCA59077.2024.00069,https://doi.org/10.1109/ISCA59077.2024.00069,False,False,True,ISCA,ACM/IEEE 51st Annual International Symposium on Computer Architecture,DEADBEEF
2024,IEEE,article,Design,"['VIT', 'Transformer']",__no_data__,A 109-GOPs/W FPGA-based Vision Transformer Accelerator with Weight-Loop Dataflow Featuring Data Reusing and Resource Saving,https://doi.org/10.1109/TCSVT.2024.3439600,https://doi.org/10.1109/TCSVT.2024.3439600,False,False,True,,IEEE Transactions on Circuits and Systems for Video Technology ( Early Access ),DEADBEEF
2024,IEEE,article,Design,"['VIT', 'Transformer']",__no_data__,Klotski v2: Improved DNN Model Orchestration Framework for Dataflow Architecture Accelerators,https://doi.org/10.1109/TCAD.2024.3446858,https://doi.org/10.1109/TCAD.2024.3446858,False,False,True,,IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems,DEADBEEF
2024,Springer,article,Design,"['VIT', 'Transformer']",__no_data__,Quartet: A Holistic Hybrid Parallel Framework for Training Large Language Models,https://doi.org/10.1007/978--3--031--69766--1_29,https://doi.org/10.1007/978-3-031-69766-1_29,False,False,True,,European Conference on Parallel and Distributed Processing,DEADBEEF
2024,Springer,article,Design,"['VIT', 'Transformer']",__no_data__,Inference with Transformer Encoders on ARM and RISC-V Multicore Processors,https://doi.org/10.1007/978--3--031--69766--1_26,https://doi.org/10.1007/978-3-031-69766-1_26,False,False,True,,European Conference on Parallel and Distributed Processing,DEADBEEF
2024,ACM,article,Design,"['VIT', 'Transformer']",__no_data__,Mentor: A Memory-Eficient Sparse-dense Matrix Multiplication Accelerator Based on Column-Wise Product,https://dl.acm.org/doi/pdf/10.1145/3688612,https://dl.acm.org/doi/pdf/10.1145/3688612,False,False,True,,ACM Transactions on Architecture and Code Optimization,DEADBEEF
2024,IEEE,article,Design,"['VIT', 'Transformer']",__no_data__,Cost-Effective LLM Accelerator Using Processing in Memory Technology,https://doi.org/10.1109/VLSITechnologyandCir46783.2024.10631397,https://doi.org/10.1109/VLSITechnologyandCir46783.2024.10631397,False,False,True,,IEEE Symposium on VLSI Technology and Circuits (VLSI Technology and Circuits),DEADBEEF
2024,IEEE,article,Design,"['VIT', 'Transformer']",__no_data__,"A 28nm 4.35TOPS/mm2 Transformer Accelerator with Basis-vector Based Ultra Storage Compression, Decomposed Computation and Unified LUT-Assisted Cores",https://doi.org/10.1109/VLSITechnologyandCir46783.2024.10631311,https://doi.org/10.1109/VLSITechnologyandCir46783.2024.10631311,False,False,True,,IEEE Symposium on VLSI Technology and Circuits (VLSI Technology and Circuits),DEADBEEF
2024,ACM,article,Design,"['VIT', 'Transformer']",__no_data__,FPGA-Based Sparse Matrix Multiplication Accelerators: From State-of-the-art to Future Opportunities,https://doi.org/10.1145/3687480,https://doi.org/10.1145/3687480,https://dl.acm.org/doi/pdf/10.1145/3687480,False,True,,ACM Transactions on Reconfigurable Technology and Systems,DEADBEEF
2024,IEEE,article,platform,"['LLm', 'Transformer']",__no_data__,LPU: A Latency-Optimized and Highly Scalable Processor for Large Language Model Inference,https://doi.ieeecomputersociety.org/10.1109/MM.2024.3420728,https://doi.ieeecomputersociety.org/10.1109/MM.2024.3420728,False,False,True,ISCAS,IEEE Journal of Solid-State Circuits,DEADBEEF
2024,ACM,article,Design,"['FPGA', 'LLM']",__no_data__,Efficient Transformer Acceleration via Reconfiguration for Encoder and Decoder Models and Sparsity-Aware Algorithm Mapping,https://doi.org/10.1145/3665314.3670798,https://doi.org/10.1145/3665314.3670798,https://dl.acm.org/doi/pdf/10.1145/3665314.3670798,False,True,,ACM/IEEE International Symposium on Low Power Electronics and Design,DEADBEEF
2024,ACM,article,Design,"['DSA', 'ViT']",__no_data__,VisionAGILE: A Versatile Domain-Specific Accelerator for Computer Vision Tasks,https://doi.ieeecomputersociety.org/10.1109/TPDS.2024.3466891,https://doi.ieeecomputersociety.org/10.1109/TPDS.2024.3466891,False,False,True,,IEEE Transactions on Parallel and Distributed Systems,DEADBEEF
2024,Arxive,article,Design,"['Chiplet', 'LLM']",__no_data__,Cambricon-LLM: A Chiplet-Based Hybrid Architecture for On-Device Inference of 70B LLM,https://arxiv.org/abs/2409.15654,https://arxiv.org/abs/2409.15654,https://arxiv.org/pdf/2409.15654,False,True,,Computer Science > Hardware Architecture,DEADBEEF
2024,Arxive,article,Design,"['FPGA', 'LLM']",__no_data__,FAMOUS: Flexible Accelerator for the Attention Mechanism of Transformer on UltraScale+ FPGAs,https://arxiv.org/abs/2409.14023,https://arxiv.org/abs/2409.14023,https://arxiv.org/pdf/2409.14023,False,True,,Computer Science > Hardware Architecture,DEADBEEF
2024,Arxive,article,Design,"['ASIC', 'LLM']",__no_data__,Hardware-oriented algorithms for softmax and layer normalization of large language models,https://doi.org/10.1007/s11432--024--4137--4,https://doi.org/10.1007/s11432-024-4137-4,https://arxiv.org/pdf/2409.14023,False,True,SciEngine,SCIENCE CHINA Information Sciences,DEADBEEF
2024,Arxive,article,Design,"['FPGA', 'LLM']",__no_data__,Optimizing DNN Inference on Multi-Accelerator SoCs at Training-time,https://arxiv.org/abs/2409.18566,https://arxiv.org/abs/2409.18566,https://arxiv.org/pdf/2409.18566,False,True,,Computer Science > Machine Learning,DEADBEEF
2024,Arxive,article,Design,"['FPGA', 'LLM']",__no_data__,Optimizing DNN Inference on Multi-Accelerator SoCs at Training-time,https://arxiv.org/abs/2409.18566,https://arxiv.org/abs/2409.18566,https://arxiv.org/pdf/2409.18566,False,True,,Computer Science > Machine Learning,DEADBEEF
2024,IEEE,article,Design,"['GPU', 'LLM']",__no_data__,DSTC: Dual-Side Sparse Tensor Core for DNNs Acceleration on Modern GPU Architectures,https://doi.ieeecomputersociety.org/10.1109/TC.2024.3475814,https://doi.ieeecomputersociety.org/10.1109/TC.2024.3475814,False,False,True,,IEEE Transactions on Computers,DEADBEEF
2024,IEEE,article,Design,"['DSA', 'ViT']",__no_data__,Power Efficient ASIC Design for Vision Transformer using Systolic Array Matrix Multiplier,https://doi.org/10.1109/VDAT63601.2024.10705728,https://doi.org/10.1109/VDAT63601.2024.10705728,False,False,True,VDAT,International Symposium on VLSI Design and Test,DEADBEEF
2024,Arxive,article,Design,"['FPGA', 'LLM']",__no_data__,M^2-ViT: Accelerating Hybrid Vision Transformers with Two-Level Mixed Quantization,https://arxiv.org/abs/2410.09113,https://arxiv.org/abs/2410.09113,https://arxiv.org/pdf/2410.09113,False,True,,Computer Science > Hardware Architecture,DEADBEEF
2024,ACM,article,Design,"['GPU', 'LLM']",__no_data__,A Cascaded ReRAM-based Crossbar Architecture for Transformer Neural Network Acceleration,https://doi.org/10.1145/3701034,https://doi.org/10.1145/3701034,https://dl.acm.org/doi/pdf/10.1145/3701034,False,True,,ACM Transactions on Design Automation of Electronic Systems,DEADBEEF
2024,IEEE,article,Design,"['GPU', 'LLM']",__no_data__,OPASCA: Outer Product Based Accelerator With Unified Architecture for Sparse Convolution and Attention,https://doi.org/10.1109/TCAD.2024.3483092,https://doi.org/10.1109/TCAD.2024.3483092,False,False,True,,IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems,DEADBEEF
2024,IEEE,article,Design,"['Algorithm', 'LLM']",__no_data__,HotaQ: Hardware Oriented Token Adaptive Quantization for Large Language Models,https://doi.org/10.1109/TCAD.2024.3487781,https://doi.org/10.1109/TCAD.2024.3487781,False,False,True,,IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems,DEADBEEF
2024,IEEE,article,Design,"['RISC-V', 'LLM']",__no_data__,Analysis Towards Deployment and Acceleration for ViT on a Lightweight RISC- V Processor,https://doi.org/10.1109/IAI63275.2024.10730301,https://doi.org/10.1109/IAI63275.2024.10730301,False,False,True,,International Conference on Industrial Artificial Intelligence (IAI)  ,DEADBEEF
2024,IEEE,article,Design,"['Algorithm', 'LLM']",__no_data__,Improving Transformer Inference Through Optimized Non-Linear Operations With Quantization-Approximation-Based Strategy,https://doi.org/10.1109/TCAD.2024.3488572,https://doi.org/10.1109/TCAD.2024.3488572,False,False,True,,IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems,DEADBEEF
2024,IEEE,article,Design,"['DSA', 'LLM']",__no_data__,HyCTor: A Hybrid CNN-Transformer Network Accelerator With Flexible Weight/Output Stationary Dataflow and Multi-Core Extension,https://doi.org/10.1109/TCAD.2024.3490173,https://doi.org/10.1109/TCAD.2024.3490173,False,False,True,,IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems,DEADBEEF
2024,Arxive,article,Design,"['FPGA', 'LLM']",__no_data__,Shrinking the Giant : Quasi-Weightless Transformers for Low Energy Inference,https://arxiv.org/abs/2411.01818,https://arxiv.org/abs/2411.01818,https://arxiv.org/pdf/2411.01818,False,True,,Computer Science > Machine Learning,DEADBEEF
2024,Arxive,article,Design,"['FPGA', 'LLM']",__no_data__,Multilayer Dataflow based Butterfly Sparsity Orchestration to Accelerate Attention Workloads,https://arxiv.org/abs/2411.00734,https://arxiv.org/abs/2411.00734,https://arxiv.org/pdf/2411.00734,False,True,,Computer Science > Hardware Architecture,DEADBEEF
2024,Arxive,article,Design,"['FPGA', 'LLM']",__no_data__,TATAA: Programmable Mixed-Precision Transformer Acceleration with a Transformable Arithmetic Architecture,https://arxiv.org/abs/2411.03697,https://arxiv.org/abs/2411.03697,https://arxiv.org/pdf/2411.03697,False,True,,Computer Science > Hardware Architecture,DEADBEEF
2024,IEEE,article,Design,"['DSA', 'LLM']",__no_data__,Exploring Approximation and Dataflow Co-Optimization for Scalable Transformer Inference Architecture on the Edge,https://doi.org/10.1109/SOCC62300.2024.10737793,https://doi.org/10.1109/SOCC62300.2024.10737793,False,False,True,SOCC,International System-on-Chip Conference,DEADBEEF
2024,IEEE,article,Design,"['FPGA', 'ViT']",__no_data__,Vision Transformer Inference on a CNN Accelerator,https://doi.org/10.1109/ICCD63220.2024.00101,https://doi.org/10.1109/ICCD63220.2024.00101,False,False,True,ICCD,IEEE 42nd International Conference on Computer Design,DEADBEEF
2024,Arxive,article,Design,"['FPGA', 'LLM']",__no_data__,Addressing Architectural Obstacles for Overlay with Stream Network Abstraction,https://arxiv.org/abs/2411.17966,https://arxiv.org/abs/2411.17966,https://arxiv.org/pdf/2411.17966,False,True,,Computer Science > Hardware Architecture,DEADBEEF
2024,Arxive,article,Design,"['FPGA', 'LLM']",__no_data__,A Runtime-Adaptive Transformer Neural Network Accelerator on FPGAs,https://arxiv.org/abs/2411.18148,https://arxiv.org/abs/2411.18148,https://arxiv.org/pdf/2411.18148,False,True,,Computer Science > Hardware Architecture,DEADBEEF
2024,Arxive,article,Design,"['FPGA', 'LLM']",__no_data__,A Dataflow Compiler for Efficient LLM Inference using Custom Microscaling Formats,https://arxiv.org/abs/2307.15517,https://arxiv.org/abs/2307.15517,https://arxiv.org/pdf/2307.15517,False,True,,Computer Science > Hardware Architecture,DEADBEEF
2024,IEEE,article,Design,"['DSA', 'LLM']",__no_data__,MR-Transformer: FPGA Accelerated Deep Learning Attention Model for Modulation Recognition,https://doi.org/10.1109/TWC.2024.3506743,https://doi.org/10.1109/TWC.2024.3506743,False,False,True,SOCC,IEEE Transactions on Wireless Communications,DEADBEEF
2024,ACM,article,Design,"['PIM', 'LLM']",__no_data__,ISOAcc: In-situ Shift Operation-based Accelerator For Efficient in-SRAM Multiplication,https://doi.org/10.1145/3707205,https://doi.org/10.1145/3707205,False,False,True,,ACM Transactions on Design Automation of Electronic Systems,DEADBEEF
2024,ACM,article,Design,"['PIM', 'LLM']",__no_data__,VGA: Hardware Accelerator for Scalable Long Sequence Model Inference,https://doi.ieeecomputersociety.org/10.1109/MICRO61859.2024.00106,https://doi.ieeecomputersociety.org/10.1109/MICRO61859.2024.00106,False,False,True,MICRO,IEEE/ACM International Symposium on Microarchitecture,DEADBEEF
2024,ACM,article,Design,"['FPGA', 'ViT']",__no_data__,Hardware Accelerated Vision Transformer via Heterogeneous Architecture Design and Adaptive Dataflow Mapping,https://doi.ieeecomputersociety.org/10.1109/TC.2024.3517751,https://doi.ieeecomputersociety.org/10.1109/TC.2024.3517751,False,False,True,TC,IEEE Transactions on Computers,DEADBEEF
2024,Arxiv,article,Design,"['PIM', 'GPT']",__no_data__,IMTP: Search-based Code Generation for In-memory Tensor Programs,https://arxiv.org/abs/2412.19630,https://arxiv.org/abs/2412.19630,https://arxiv.org/pdf/2412.19630,False,True,HA,Computer Science > Hardware Architecture,DEADBEEF
2024,Arxiv,article,Design,"['GPGPU', 'LLM']",__no_data__,Debunking the CUDA Myth Towards GPU-based AI Systems,https://arxiv.org/abs/2501.00210,https://arxiv.org/abs/2501.00210,https://arxiv.org/pdf/2501.00210,False,True,HA,"Computer Science > Distributed, Parallel, and Cluster Computing",DEADBEEF
2025,Arxiv,article,Design,"['CPU', 'LLM']",__no_data__,MixGCN: Scalable GCN Training by Mixture of Parallelism and Mixture of Accelerators,https://arxiv.org/abs/2501.01951,https://arxiv.org/abs/2501.01951,https://arxiv.org/pdf/2501.01951,False,True,,Computer Science > Machine Learning,DEADBEEF
2025,Arxiv,article,Design,"['FPGA', 'LLM']",__no_data__,Ultra Memory-Efficient On-FPGA Training of Transformers via Tensor-Compressed Optimization,https://arxiv.org/abs/2501.06663,https://arxiv.org/abs/2501.06663,https://arxiv.org/pdf/2501.06663,False,True,,Computer Science > Machine Learning,DEADBEEF
2025,Arxiv,article,Design,"['FPGA', 'LLM']",__no_data__,An Efficient Sparse Hardware Accelerator for Spike-Driven Transformer,https://arxiv.org/abs/2501.07825,https://arxiv.org/abs/2501.07825,https://arxiv.org/pdf/2501.07825,False,True,,Computer Science > Hardware Architecture,DEADBEEF
2025,IEEE,article,Design,"['FPGA', 'LLM']",__no_data__,MPTorch-FPGA: a Custom Mixed-Precision Framework for FPGA-based DNN Training,https://hal.science/hal--04882989v2,https://hal.science/hal-04882989v2,https://hal.science/hal-04882989v2,False,True,DATE,"EEE/ACM Design, Automation and Test in Europe",DEADBEEF
2025,Arxiv,article,Design,"['FPGA', 'LLM']",__no_data__,Atleus: Accelerating Transformers on the Edge Enabled by 3D Heterogeneous Manycore Architectures,https://arxiv.org/abs/2501.09588,https://arxiv.org/abs/2501.09588,https://arxiv.org/pdf/2501.09588,False,True,,Computer Science > Hardware Architecture,DEADBEEF
2025,Arxiv,article,Design,"['FPGA', 'LLM']",__no_data__,Stream-HLS: Towards Automatic Dataflow Acceleration,https://arxiv.org/abs/2501.09118,https://arxiv.org/abs/2501.09118,https://arxiv.org/pdf/2501.09118,False,True,,Computer Science > Hardware Architecture,DEADBEEF
2025,IEEE,article,Design,"['FPGA', 'LLM']",__no_data__,An Efficient Window-Based Vision Transformer Accelerator via Mixed-Granularity Sparsity,https://doi.org/10.1109/TCSI.2025.3527541,https://doi.org/10.1109/TCSI.2025.3527541,False,False,True,,IEEE Transactions on Circuits and Systems I: Regular Papers,DEADBEEF
2025,IEEE,article,Design,"['FPGA', 'LLM']",__no_data__,Hardware Accelerator for Bidirectional Encoder Representations from Transformers (BERT),https://doi.org/10.1109/ICM63406.2024.10815822,https://doi.org/10.1109/ICM63406.2024.10815822,False,False,True,,IEEE International Conference on Microelectronics (ICM),DEADBEEF
2025,IEEE,article,Design,"['FPGA', 'LLM']",__no_data__,Fine-Grained Structured Sparse Computing for FPGA-Based AI Inference,https://doi.org/10.1109/TCAD.2024.3524356,https://doi.org/10.1109/TCAD.2024.3524356,False,False,True,,IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems,DEADBEEF
2024,IEEE,article,Design,"['FPGA', 'LLM']",__no_data__,A Fully-Integrated Energy-Scalable Transformer Accelerator for Language Understanding on Edge Devices,https://doi.org/10.1109/OJSSCS.2024.3524359,https://doi.org/10.1109/OJSSCS.2024.3524359,False,False,True,,IEEE Open Journal of the Solid-State Circuits Society,DEADBEEF
2024,RoyalSociety,article,Design,"['FPGA', 'LLM']",__no_data__,AxLaM: energy-efficient accelerator design for language models for edge computing,https://doi.org/10.1098/rsta.2023.0395,https://doi.org/10.1098/rsta.2023.0395,https://doi.org/10.1098/rsta.2023.0395,False,True,,Emerging technologies for future secure computing platforms,DEADBEEF
2025,HAL,article,Design,"['FPGA', 'LLM']",__no_data__,An ultra-low-power CGRA for accelerating Transformers at the edge,https://hal.science/hal--04914400v1,https://hal.science/hal-04914400v1,https://hal.science/hal-04914400v1,False,True,DATE,HAL open science,DEADBEEF
2025,Arxiv,article,Design,"['DLA', 'LLM']",__no_data__,A Tensor-Train Decomposition based Compression of LLMs on Group Vector Systolic Accelerator,https://arxiv.org/abs/2501.19135,https://arxiv.org/abs/2501.19135,https://arxiv.org/pdf/2501.19135,False,True,,Computer Science > Hardware Architecture,DEADBEEF
2025,Arxiv,article,Design,"['PIM', 'LLM']",__no_data__,Towards Efficient LUT-based PIM: A Scalable and Low-Power Approach for Modern Workloads,https://arxiv.org/abs/2502.02142,https://arxiv.org/abs/2502.02142,https://arxiv.org/pdf/2502.02142,False,True,,Computer Science > Hardware Architecture,DEADBEEF
2025,Arxiv,article,Design,"['DLA', 'LLM']",__no_data__,Systolic Sparse Tensor Slices: FPGA Building Blocks for Sparse and Dense AI Acceleration,https://arxiv.org/abs/2502.03763,https://arxiv.org/abs/2502.03763,https://arxiv.org/pdf/2502.03763,False,True,,Computer Science > Hardware Architecture,DEADBEEF
2025,Arxiv,article,Design,"['Optoc', 'LLM']",__no_data__,Hybrid Photonic-digital Accelerator for Attention Mechanism,https://arxiv.org/abs/2501.11286,https://arxiv.org/abs/2501.11286,https://arxiv.org/pdf/2501.11286,False,True,,Computer Science > Hardware Architecture,DEADBEEF
2025,Arxiv,article,Design,"['SoC', 'LLM']",__no_data__,HeteroLLM: Accelerating Large Language Model Inference on Mobile SoCs platform with Heterogeneous AI Accelerators,https://arxiv.org/abs/2501.14794,https://arxiv.org/abs/2501.14794,https://arxiv.org/pdf/2501.14794,False,True,,Computer Science > Hardware Architecture,DEADBEEF
2025,Elsevier,article,Design,"['SoC', 'ViT']",__no_data__,DRViT: A dynamic redundancy-aware vision transformer accelerator via algorithm and architecture co-design on FPGA,https://doi.org/10.1016/j.jpdc.2025.105042,https://doi.org/10.1016/j.jpdc.2025.105042,False,False,True,,Journal of Parallel and Distributed Computing,DEADBEEF
